\OHPC{} provides multiple options for distributed resource management. 
The following command adds the \SLURM{} workload manager server components to the
chosen {\em master} host. Note that client-side components will be added to
the corresponding compute image in a subsequent step.

% begin_ohpc_run
% ohpc_comment_header Add resource management services on master node \ref{sec:add_rm}
\begin{lstlisting}[language=bash,keywords={}]
# Install slurm server meta-package
[sms](*\#*) (*\install*) ohpc-slurm-server

# Use ohpc-provided file for starting SLURM configuration
[sms](*\#*) cp /etc/slurm/slurm.conf.ohpc /etc/slurm/slurm.conf
# Setup default cgroups file
[sms](*\#*) cp /etc/slurm/cgroup.conf.example /etc/slurm/cgroup.conf

# Identify resource manager hostname on master host
[sms](*\#*) perl -pi -e "s/SlurmctldHost=\S+/SlurmctldHost=${sms_name}/" /etc/slurm/slurm.conf
\end{lstlisting}
% end_ohpc_run

There are a wide variety of configuration options and plugins available
for \SLURM{} and the example config file illustrated above targets a fairly
basic installation. In particular, job completion data will be stored in a text
file (\texttt{/var/log/slurm\_jobcomp.log)} that can be used to log simple
accounting information. Sites who desire more detailed information, or want to
aggregate accounting data from multiple clusters, will likely want to enable the
database accounting back-end.  This requires a number of additional local modifications
(on top of installing \texttt{slurm-slurmdbd-ohpc}), and users are advised to
consult the online \href{https://slurm.schedmd.com/accounting.html}{\color{blue}{documentation}}
for more detailed information on setting up a database configuration for \SLURM{}.

\begin{center}
\begin{tcolorbox}[]
  \small SLURM requires enumeration of the physical hardware characteristics for
  compute nodes under its control. In particular, three configuration parameters
  combine to define consumable compute resources: {\em Sockets}, {\em
  CoresPerSocket}, and {\em ThreadsPerCore}. The default configuration file
  provided via \OHPC{} assumes the nodes are named c1-c4 and are dual-socket, 8
  cores per socket, and two threads per core for this 4-node example. If this
  does not reflect your local hardware, please update the configuration file at
  \path{/etc/slurm/slurm.conf} accordingly to match your nodes names and
  particular hardware. Be sure to run \texttt{scontrol reconfigure} to notify
  SLURM of the changes. Note that the SLURM project provides an easy-to-use
  online configuration tool that can be accessed
  \href{https://slurm.schedmd.com/configurator.html}{\color{blue} here}.
\end{tcolorbox}
\end{center}

% begin_ohpc_run
% ohpc_comment_header Update node configuration for slurm.conf
% ohpc_command if [[ ${update_slurm_nodeconfig} -eq 1 ]];then
% ohpc_indent 5
% ohpc_command perl -pi -e "s/^NodeName=.+$/NodeName=${slurm_node_config}/" /etc/slurm/slurm.conf
% ohpc_command perl -pi -e "s/ Nodes=c\S+ / Nodes=c[1-$num_computes] /" /etc/slurm/slurm.conf
% ohpc_indent 0
% ohpc_command fi
% end_ohpc_run

Other versions of this guide are available that describe installation of alternate
resource management systems, and they can be found in the \texttt{docs-ohpc}
package.

