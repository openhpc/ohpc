\OHPC{} provides multiple options for distributed resource management. 
The following command adds the \SLURM{} workload manager server components to the
chosen {\em master} host. Note that client-side components will be added to
the corresponding compute image in a subsequent step.

% begin_ohpc_run
% ohpc_comment_header Add resource management services on master node \ref{sec:add_rm}
\begin{lstlisting}[language=bash,keywords={}]
[sms](*\#*) (*\install*) ohpc-slurm-server
# Identify resource manager hostname on master host
[sms](*\#*) perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
\end{lstlisting}
% end_ohpc_run

\begin{center}
\begin{tcolorbox}[]
  \small SLURM requires enumeration of the physical hardware characteristics
  for compute nodes under its control. In particular, three configuration
  parameters combine to define consumable compute resources: {\em Sockets},
  {\em CoresPerSocket}, and {\em ThreadsPerCore}. The default configuration
  file provided via \OHPC{} assumes dual-socket, 8 cores per socket, and two
  threads per core for this 4-node example. If this does not reflect your local
  hardware, please update the configuration file at
  \path{/etc/slurm/slurm.conf} accordingly to match your particular hardware.
\end{tcolorbox}
\end{center}

Other versions of this guide are available that describe installation of other
resource management systems, and they can be found in the \texttt{docs-ohpc}
package.

