\documentclass[letterpaper]{article}
\usepackage{../../common/fspdoc}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\lhead{{2015 \FSP{} Install Guide - {\small \em SLES12 Version}}}


% Recipe commands to run initially only in CI

% begin_fsp_run 
% fsp_validation_comment - Remove WW file update crontab entry (CI only command)
% [master]$ rm -f /etc/cron.d/wwupdatefiles
% [master]$ TIME_T1=$(date +%s)
% [master]$ echo " "; echo "BEGIN TIMING"
% end_fsp_run

% Include git variables
\input{vc.tex}

% Define Base OS
\newcommand{\baseOS}{SLES12*}
\newcommand{\LosF}{\emph{LosF*}}
\newcommand{\FSPVersion}{15.16}

\begin{document}
\graphicspath{{../../common/figures/}}
\thispagestyle{empty}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

% Title Page

{\hspace*{4in} \includegraphics[width=1.8in]{intel_1spot_100.pdf}}

\vspace*{2cm}
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont 2015 Forest Peak (v\FSPVersion{})} \vspace*{0.1cm} \\
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont Cluster Building Recipes} \\ 
\noindent\rule{6in}{0.4pt} \\

\noindent {\Large \baseOS{} Base OS} \\ \vspace{0.2cm}

\noindent {\large {\em Base Linux* Edition }}

\vspace*{3in}

\noindent{\normalsize Intel Cluster Makers} \vspace*{0.1cm} \\
{\normalsize Copyright~{\small\copyright}~2014-2015 Intel Corporation} \vspace*{0.1cm} \\ 
{\normalsize Document Last Update: \VCDateISO} \vspace*{0.1cm} \\ 
{\normalsize Document Revision: \VCRevision} \\ \vspace*{0.1cm}

% Disclaimer  ----------------------------------------------------
\input{../../common/legal} 

\newpage
\tableofcontents
\newpage

% Introduction  --------------------------------------------------

\section{Introduction} \label{sec:introduction}
\input{../../common/intro} \\

\input{../../common/base_edition/edition}
\input{../../common/audience.tex}
\input{../../common/requirements}
\input{../../common/inputs}

% Base Operating System --------------------------------------------

\section{Install Base Operating System (BOS)}
\input{../../common/bos}

% ------------------------------------------------------------------

\section{Install \FSP{} Components} \label{sec:basic_install}
\input{../../common/install_fsp_component_intro.tex}

\subsection{Enable \FSP{} repository for local use}
\input{../../common/enable_fsp_repo}

% begin_fsp_run
% fsp_validation_comment Enable FSP Repo(s)
\begin{lstlisting}[language=bash,keywords={}]
[master]$ export FSP_mirror=http://fsp-obs.pdx.intel.com:82
[master]$ zypper addrepo $FSP_mirror/ForestPeak:/15.16/SLE_12_Intel/ForestPeak:15.16.repo
[master]$ zypper -n --gpg-auto-import-keys refresh
\end{lstlisting}
% end_fsp_run

% Dev testing for Factory
%[master]$ zypper addrepo $FSP_mirror/ForestPeak:/15.16:/Factory/SLE_12_Intel/ForestPeak:15.16:Factory.repo

In addition to the \FSP{} package repository, the {\em master} host also
requires access to the standard distro repositories in order to resolve
necessary dependencies. For \baseOS{}, the requirements are to have access to
both the base and SDK repositories:

\begin{itemize*}
\item SLES12-12-0
\item SLES12-12-sdk
\end{itemize*}

% begin_fsp_run
% % fsp_validation_comment - format space for /opt/fsp (CI only command)
% [master]$ mkfs -F -t ext4 /dev/sda1
% [master]$ mkdir -p /opt/fsp
% [master]$ mount /dev/sda1 /opt/fsp
% end_fsp_run  

\subsection{Add provisioning services on {\em master} node}
\input{../../common/install_provisioning_intro}

% begin_fsp_run
% fsp_validation_comment Add baseline FSP and warewulf on master node
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern fsp-base      
[master]$ zypper -n install -t pattern fsp-warewulf
\end{lstlisting}
% end_fsp_run

\subsection{Add resource management services on {\em master} node} \label{sec:add_rm}

The following command adds the \SLURM{} workload manager server components to the
chosen {\em master} host. Note that client-side components will be added to
the corresponding compute image in a subsequent step.

% begin_fsp_run
% fsp_validation_comment Add resource manager
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern fsp-slurm-server
\end{lstlisting}
% end_fsp_run

\SLURM{} requires the delineation of the system user that runs the underlying
resource management daemons. The default configuration file that is supplied
with the \FSP{} build of \SLURM{} identifies this \texttt{SlurmUser} to be a
dedicated user named \texttt{slurm} and this user must exist. 
%The default location identified to save state information is
%\texttt{/var/slurm} and this directory
The following command can be used to add this user to the {\em
  master} server:

% begin_fsp_run
% fsp_validation_comment Add slurm user
\begin{lstlisting}[language=bash,keywords={}]
[master]$ useradd slurm
\end{lstlisting}
% end_fsp_run

\newpage
To facilitate running test jobs later on under the auspices of a resource
manager, we also add a dedicated ``test'' user as follows:

% begin_fsp_run
% fsp_validation_comment Add test user
\begin{lstlisting}[language=bash,keywords={}]
[master]$ useradd -m test
\end{lstlisting}
% end_fsp_run

\subsection{Add \InfiniBand{} support services on {\em master} node} \label{sec:add_ofed}

The following command adds OFED support using distro-provided drivers to the
chosen {\em master} host. 

% begin_fsp_run
% fsp_validation_comment Add ofed
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install libibverbs-runtime libmlx4-rdmav2
[master]$ zypper -n install libibmad5 librdmacm1 rdma

# Load IB drivers
[master]$ service rdma start
\end{lstlisting}
% end_fsp_run

With the \InfiniBand{} drivers included, you can also enable IPoIB functionality
which provides a mechanism to send IP packets over the IB network. If you plan
to mount a \Lustre{} file system over \InfiniBand{} (see \S\ref{sec:lustre_client}
for additional details), then having IPoIB enabled is a requirement for the
\Lustre{} client. \FSP{} provides a template configuration file to aid in setting up
an {\em ib0} interface on the {\em master} host. To use, copy the template
provided and update the \texttt{master\_ipoib} and
\texttt{ipoib\_netmask} entries to match local desired settings (alter ib0
naming as appropriate if system contains dual-ported or multiple HCAs). 

% begin_fsp_run
% fsp_validation_comment Enable ib0
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ cp /opt/fsp/pub/examples/network/sles/ifcfg-ib0 /etc/sysconfig/network

# Define local IPoIB address and netmask
[master]$ perl -pi -e 's/master_ipoib/<master_ipoib>/' /etc/sysconfig/network/ifcfg-ib0
[master]$ perl -pi -e 's/ipoib_netmask/<ipoib_netmask>/' /etc/sysconfig/network/ifcfg-ib0

# Initiate ib0
[master]$ ifup ib0
\end{lstlisting}
% end_fsp_run

% Intel Cluster Checker section
\input{../../common/cluster_checker}

% begin_fsp_run
% fsp_validation_comment Add CLCK
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install intel-clck-fsp
\end{lstlisting}
% end_fsp_run


\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use \Warewulf{} on the {\em
  master} host should be installed.  Next, we need to update several
configuration files in order to allow \Warewulf{} to work with \baseOS{} and to support
local provisioning using the {\em eth1} interface.  Specific steps are as
follows:

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node
%\begin{verbatim}

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]

# Configure DHCP server to use eth1
[master]$ perl -pi -e 's/^DHCPD_INTERFACE=""/DHCPD_INTERFACE="eth1"/' /etc/sysconfig/dhcpd

# Enable tftp service for compute node image distribution
[master]$ perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp

# Configure Warewulf to use the default SLES tftp location
[master]$ perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /srv/#" /etc/warewulf/provision.conf

# Update Warewulf http config to use the SLES version of mod_perl
[master]$ export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[master]$ perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory to support newer apache syntax
[master]$ perl -pi -e "s/cgi-bin>\$/cgi-bin>\n Require all granted/" $MODFILE
[master]$ perl -pi -e "s/Allow from all/Require all granted/" $MODFILE
[master]$ perl -ni -e "print unless /^\s+Order allow,deny/" $MODFILE

# Define eth1 interface for provisioning
[master]$ ifconfig eth1 <master_ip> netmask 255.255.255.0 up

# Restart relevant services to support provisioning
[master]$ service xinetd restart                         # tftp services
[master]$ service mysql restart                          # mysql database
[master]$ service apache2 restart                        # web server
\end{lstlisting}
%\end{verbatim}
% end_fsp_run

%%% The steps above included starting the {\em eth1} interface on the {\em master}
%%% host in order to provide provisioning services locally for the cluster. In this
%%% install recipe, the {\em master} host will also provide resource manager
%%% services and requires a hostname to be associated with the {\em eth1} interface
%%% so that the compute hosts can reference it by name. Portions of the
%%% \texttt{/etc/hosts} file on the {\em master} host will be automatically
%%% propagated to the provisioned hosts and a hostname entry that maps to this {\em
%%%   eth1} interface can be added as follows:
%%% 
%%% % abegin_fsp_run
%%% \begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
%%% # add local hostname entry for eth1 
%%% [master]$ echo "<master_ip> <master_hostname>-eth1" >> /etc/hosts   
%%% 
%%% # import (sync) the updated hosts file with Warewulf
%%% [master]$ wwsh file sync dynamic_hosts
%%% \end{lstlisting}
%%% % aend_fsp_run

% begin_fsp_run 
% fsp_validation_comment - time update
% [master]$ TIME_T2=$(date +%s) ; TIME_DELTA=$(($TIME_T2-$TIME_T1)) ; TIME_T1=$TIME_T2
% [master]$ echo " "; echo "END TIMING = $TIME_DELTA (secs)"
% end_fsp_run



\subsection{Define {\em compute} image for provisioning}

With the provisioning services enabled, the next step is to define and
customize a system image that can subsequently be used to provision one or more
{\em compute} nodes. The following subsections highlight this process.

\subsubsection{Build initial BOS image}

The \FSP{} build of \Warewulf{} includes specific enhancements enabling support for
\baseOS{}. The following steps illustrate the process to build a minimal, default
image for use with \Warewulf{}.  We begin by creating a directory structure on the 
{\em master} that will represent the root filesystem of the compute node. The 
default location for this in \FSP{} is in \texttt{/opt/fsp/admin/images/sles12}.

% begin_fsp_run
% fsp_validation_comment Create compute image for Warewulf
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]
# Define chroot location for SLES 
[master]$ export CHROOT=/opt/fsp/admin/images/sles12

# Build initial chroot image

[master]$ mkdir -p -m 755 $CHROOT                        # create chroot housing dir
[master]$ mkdir -m 755 $CHROOT/dev                       # create chroot /dev dir
[master]$ mknod -m 666 $CHROOT/dev/zero c 1 5            # create /dev/zero device
[master]$ wwmkchroot sles-12 $CHROOT                     # create base image
\end{lstlisting}
% end_fsp_run

\subsubsection{Add \FSP{} components}

The \texttt{wwmkchroot} process used in the previous step is designed to
provide a minimal \baseOS{} configuration. Next, we add additional components to
include resource management client services, \InfiniBand{} drivers, and other
additional packages to support the default \FSP{} environment.  This process uses
the \texttt{chroot} command to augment the base provisioning image and will
access the BOS and \FSP{} repositories to resolve package install requests. To
access the remote repositories by hostname (and not IP addresses), the chroot
environment needs to be updated to enable DNS resolution. Assuming that
the {\em master} host has a working DNS configuration in place, the chroot environment can
be updated with a copy of the configuration as follows:

% begin_fsp_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
\end{lstlisting}
% end_fsp_run

%\newpage
% begin_fsp_run
% fsp_validation_comment Add FSP components to compute instance
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Add SLURM client support
[master]$ zypper --root $CHROOT -n install -t pattern fsp-slurm-client

# Add IB support
[master]$ zypper --root $CHROOT -n install libibverbs-runtime libmlx4-rdmav2
[master]$ zypper --root $CHROOT -n install libibmad5 librdmacm1 rdma

# Add Network Time Protocol (NTP) support
[master]$ zypper --root $CHROOT -n install ntp

# Include FSP default modules user environment
[master]$ zypper --root $CHROOT -n install lmod-defaults-intel-fsp

# Enable ssh access 
[master]$ chroot $CHROOT systemctl enable sshd.service

# Remove default hostname to allow WW to provision network names
[master]$ mv $CHROOT/etc/hostname $CHROOT/etc/hostname.orig
\end{lstlisting}
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - Restart openibd (CI only command)
% [master]$ service openibd restart
% end_fsp_run

\subsubsection{Customize system configuration} \label{sec:master_customization}

Prior to assembling the image, it is advantageous to perform any additional
customizations within the chroot environment created for the desired {\em
  compute} instance. The following steps document the process to add a local
{\em ssh} key created by \Warewulf{} to support remote access, identify the
resource manager server, configure NTP for compute resources, and enable \NFS{}
mounting of a \$HOME file system and the public \FSP{} install path
(\texttt{/opt/fsp/pub}) that will be hosted by the {\em master} host in this
example configuration.  The \NFS{} exporting options use an address/netmask
combination to limit the export scope to the defined compute nodes.

%%Note that the example shown
%%here leverages the {\em eth1} hostname identified for the {\em master} host and
%%specifically exports the desired NFS paths to each compute node
%%individually. In practice, however, a single entry would likely be used to
%%export the mount point to a private subnet suitably sized to comprise all
%%desired endpoints.

% begin_fsp_run
% fsp_validation_comment Add ssh key and enable NFS mount of /opt/fsp/pub
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# add new cluster key to base image
[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add NFS client mounts of /home and /opt/fsp/pub to base image
[master]$ echo "<master_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> $CHROOT/etc/fstab
[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

# Identify resource manager server IP in SLURM config file on computes
[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=<master_ip>/" $CHROOT/etc/slurm/slurm.conf

# Identify resource manager hostname on master host
[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=<master_hostname>/" /etc/slurm/slurm.conf

# Export /home and FSP public packages from master server to cluster compute nodes
[master]$ echo "/home <internal_netmask>(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <internal_netmask>(ro,no_subtree_check,fsid=11)" >> /etc/exports
[master]$ exportfs -a
[master]$ service nfsserver restart

# Enable NTP time service on computes and identify master host as local NTP server
[master]$ chroot $CHROOT systemctl enable ntpd
[master]$ echo "server <master_ip>" >> $CHROOT/etc/ntp.conf
\end{lstlisting}
% end_fsp_run

\newpage
\subsubsection{Additional Customizations ({\em optional})}

This section highlights common additional customizations that
can {\em optionally} be applied to the
local cluster environment. These customizations include:

\begin{itemize*}
\item Increase memlimits to support \InfiniBand{}
\item Restrict ssh access to compute resources
\item Add \Lustre{} client
\end{itemize*}

\noindent Details on the steps required for each of these customizations are
discussed further in the following sections.

\paragraph{Increase locked memory limits}

\input{../../common/memlimits} 

% begin_fsp_run
% fsp_validation_comment Update memlock on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Update memlock settings on master
[master]$ echo "* soft memlock unlimited" >> /etc/security/limits.conf
[master]$ echo "* hard memlock unlimited" >> /etc/security/limits.conf

# Update memlock settings within compute image
[master]$ echo "* soft memlock unlimited" >> $CHROOT/etc/security/limits.conf
[master]$ echo "* hard memlock unlimited" >> $CHROOT/etc/security/limits.conf
\end{lstlisting}
% end_fsp_run

%# Enable larger locked memory limits for SLURM daemon
%[master]$ echo "ulimit -l unlimited" >> $CHROOT/etc/sysconfig/slurm

\paragraph{Enable ssh control via resource manager} 

\input{../../common/slurm_pam}

% begin_fsp_run
% fsp_validation_comment Enable slurm pam module
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
\end{lstlisting}
% end_fsp_run

\paragraph{Add \Lustre{} client} \label{sec:lustre_client}

\input{../../common/lustre-client}
% SLES-specific addition
Additionally, note that your default \baseOS{} environment may not allow loading of
the necessary \Lustre{} kernel modules. Consequently, the example below includes
steps which update the {\texttt /etc/modprobe.d/10-unsupported-modules.conf}
file to allow loading of the necessary modules.

\newpage
% begin_fsp_run
% fsp_validation_comment Install Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Add Lustre client software to master host
[master]$ zypper -n install lustre-client-fsp lustre-client-fsp-modules

# Update config to allow Lustre modules to be loaded on master host
[master]$ perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     /etc/modprobe.d/10-unsupported-modules.conf
\end{lstlisting}
% end_fsp_run

% begin_fsp_run
% fsp_validation_comment Enable lustre in WW compute image
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Include Lustre client software in compute image
[master]$ zypper --root=$CHROOT -n install lustre-client-fsp lustre-client-fsp-modules

# Update config to allow Lustre modules to be loaded on compute hosts
[master]$ perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     $CHROOT/etc/modprobe.d/10-unsupported-modules.conf

# Include mount point and file system mount in compute image
[master]$ mkdir $CHROOT/mnt/lustre
[master]$ echo "<mgs_fs_name> /mnt/lustre lustre defaults,_netdev,localflock 0 0" >> $CHROOT/etc/fstab
\end{lstlisting}
% end_fsp_run

The default underlying network type used by \Lustre{} is {\em tcp}. If your
external \Lustre{} file system is to be mounted using a network type other than
tcp, additional configuration files are necessary to identify the desired
network type. The example below illustrates creation of modprobe config files
instructing \Lustre{} to use an \InfiniBand{} network with the {\em o2ib} LNET driver
attached to \texttt{ib0}. Note that these modifications are made to both the
{\em master} host and {\em compute} image.

% begin_fsp_run
% fsp_validation_comment Enable o2ib for Lustre
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
[master]$ echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
\end{lstlisting}
% end_fsp_run

With the \Lustre{} config complete, the client can be mounted on the {\em master}
host as follows:
% begin_fsp_run
% fsp_validation_comment mount Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ mkdir /mnt/lustre
[master]$ mount -t lustre -o localflock <mgs_fs_name> /mnt/lustre
\end{lstlisting}
% end_fsp_run



\paragraph{Enable ORCM RAS subsystem} 

\FSP{} includes a monitoring capability that can optionally be installed to
support unified data collection across the cluster. This monitoring system is
based on \ORCM{}, the open resilient cluster manager project, and provides a
convenient way to aggregate key node metrics including environmental sensors,
processor power, and resource utilization into a centralized database for
historical logging and analysis.  In a typical HPC configuration, \ORCM{} data
collection relies on having IPMI access to the baseboard management controller
(BMC) on each back-end compute resource. The data collection itself can either
be performed using {\em in-band} methods where a daemon is instantiated on each
compute resource or via {\em out-of-band} methods using external IPMI access on
a management network. To include all the components necessary to enable an \ORCM{}
server running on the {\em master} host, issue the following:

% begin_fsp_run
% fsp_validation_comment Install ORCM server
\begin{lstlisting}[language=bash]
[master]$ zypper -n install -t pattern fsp-orcm-server
\end{lstlisting}
% end_fsp_run

In order to coalesce sensor data into a central repository, \ORCM{} requires
data {\em aggregators} that are responsible for collecting data from one or
more monitored resources. For a small cluster, a common configuration is to
have a single aggregator defined and, in this example, we choose the {\em master} host
as the data aggregator. In order to store historical logging data, \ORCM{}
includes support to store aggregated data into a centralized database. In this
example, we will create an \ORCM{} database using {\em postgres} on the chosen
master host. The necessary config file changes and commands to create the
underlying database for \ORCM{} are highlighted below. Note that a generic DB
password of ``orcmpassword'' is used; this should be altered locally to a
site-specific credential. If the DB password is changed, a corresponding update
for the new credential should also be placed in the final
\texttt{/etc/sysconfig/orcmd} file.

% begin_fsp_run
% fsp_validation_comment Configure ORCM aggregator and DB
\begin{lstlisting}[language=bash,keywords={},upquote=true,keepspaces]
# Set master host as data aggregator
[master]$ perl -pi -e "s/aggregator_hostname/localhost/" /opt/open-rcm/etc/orcm-site.xml
[master]$ perl -pi -e "s/Servername = (<.+>)/Servername = <master_ip>/" /etc/orcmdb_psql.ini

# Update BMC credentials to match local values

[master]$ perl -pi -e "s/your-bmc-username/<bmc_username>/" /opt/open-rcm/etc/openmpi-mca-params.conf
[master]$ perl -pi -e "s/your-bmc-password/<bmc_password>/" /opt/open-rcm/etc/openmpi-mca-params.conf

# Initialize postgres DB driver
[master]$ odbcinst -i -d -f /etc/psql_odbc_driver.ini
[master]$ odbcinst -i -s -h -f /etc/orcmdb_psql.ini 

# Initialize postgres
[master]$ service postgresql start   

# Update postgres config files using ORCM templates
[master]$ cp /etc/postgresql.orcm.conf /var/lib/pgsql/data/postgresql.conf
[master]$ cp /etc/pg_hba.orcm.conf /var/lib/pgsql/data/pg_hba.conf

# Restart postgres
[master]$ service postgresql restart 

# Create ORCM DB
[master]$ sudo -u postgres psql --command "CREATE USER orcmuser WITH SUPERUSER PASSWORD 'orcmpassword';"
[master]$ sudo -u postgres createdb -O orcmuser orcmdb
[master]$ sudo -u postgres psql --username=orcmuser --dbname=orcmdb -f /etc/orcmdb_psql.sql

# Start ORCM on master host (requires both orcmsched and orcmd)
[master]$ systemctl start orcmsched
[master]$ systemctl start orcmd
\end{lstlisting}
% end_fsp_run

To use the {\em in-band} method of collecting data from compute resources, the
\ORCM{} client daemon is required to be available on each compute node. This can
be added to the compute image as follows:

% begin_fsp_run
% fsp_validation_comment Install ORCM client on compute image
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ zypper --root=$CHROOT -n install -t pattern fsp-orcm-client

# Set master host as data aggregator
[master]$ perl -pi -e "s/aggregator_hostname/<master_ip>/" $CHROOT/opt/open-rcm/etc/orcm-site.xml

# Enable orcmd client daemon on boot
[master]$ chroot $CHROOT systemctl enable orcmd
\end{lstlisting}
% end_fsp_run

\subsubsection{Import files} \label{sec:file_import}

The \Warewulf{} system includes functionality to import arbitrary files from the
provisioning server for distribution to managed hosts. This is one way
to distribute user credentials across to {\em compute} hosts. To
import local file-based credentials, issue the following:

% begin_fsp_run
% fsp_validation_comment Import credentials
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwsh file import /etc/passwd                                                          
[master]$ wwsh file import /etc/group
[master]$ wwsh file import /etc/shadow 
\end{lstlisting}
% \end_fsp_run

\newpage
Similarly, to import the cryptographic key that is required by the {\em munge}
authentication library to be available on every host in the resource management
pool, issue the following:

% begin_fsp_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwsh file import /etc/munge/munge.key
\end{lstlisting}
% \end_fsp_run

Finally, to add support for controlling IPoIB interfaces, \FSP{} includes a
template file for \Warewulf{} that can be imported and used later to provision
\texttt{ib0} network settings.

% begin_fsp_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwsh file import /opt/fsp/pub/examples/network/sles/ifcfg-ib0.ww
[master]$ wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network/ifcfg-ib0
\end{lstlisting}
% \end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - time update
% [master]$ TIME_T2=$(date +%s) ; TIME_DELTA=$(($TIME_T2-$TIME_T1)) ; TIME_T1=$TIME_T2
% [master]$ echo " "; echo "END TIMING = $TIME_DELTA (secs)"
% end_fsp_run

\subsection{Assemble bootstrap image}

\Warewulf{} requires a bootstrap image that is used to initialize the provisioning
process. This bootstrap image includes the runtime kernel and associated
modules. The following commands highlight the inclusion of additional drivers
and creation of the bootstrap image based on the running kernel.

% begin_fsp_run
% fsp_validation_comment Create bootstrap image for Warewulf
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# (Optional) Include Lustre drivers; needed if enabling Lustre client on computes
[master]$ export WW_CONF=/etc/warewulf/bootstrap.conf
[master]$ echo "drivers += updates/kernel/" >> $WW_CONF

# Build bootstrap image
[master]$ wwbootstrap `uname -r`
\end{lstlisting}
% end_fsp_run

\subsubsection{Assemble Virtual Node File Sytem (VNFS) image}

With the local site customizations in place, the following step uses the
\texttt{wwvnfs} command to assemble a VNFS capsule from the chroot environment
defined for the {\em compute} instance. 

% begin_fsp_run
% fsp_validation_comment Assemble VNFS
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwvnfs -y --chroot $CHROOT
\end{lstlisting}
% end_fsp_run

\subsubsection{Register nodes for provisioning}

In preparation for provisioning, we can now define the desired network settings
for four example compute nodes with the underlying provisioning system and
restart the \texttt{dhcp} service. Note the use of variable names for the
desired compute node IP and MAC addresses which should be modified to
accommodate local settings and hardware. Included in these steps are commands
to enable \Warewulf{} to manage IPoIB settings and corresponding definitions of
IPoIB addresses for the compute nodes. This is typically optional unless you
are planning to include a \Lustre{} client mount over \InfiniBand{}.  The final step
in this process associates the VNFS image assembled in previous steps with the
newly defined compute nodes, utilizing the user credential files and munge key
that were imported in \S\ref{sec:file_import}.

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Define four compute nodes and network settings 
[master]$ wwsh -y node new c1 --ipaddr=<c1_ip> --hwaddr=<c1_mac> 
[master]$ wwsh -y node new c2 --ipaddr=<c2_ip> --hwaddr=<c2_mac> 
[master]$ wwsh -y node new c3 --ipaddr=<c3_ip> --hwaddr=<c3_mac> 
[master]$ wwsh -y node new c4 --ipaddr=<c4_ip> --hwaddr=<c4_mac> 

# Define provisioning image for hosts
[master]$ wwsh -y provision set c[1-4] --vnfs=sles12 --bootstrap=`uname -r` \
     --files=dynamic_hosts,passwd,group,shadow,munge.key 

# Optionally define IPoIB network settings (required if planning to mount Lustre* over IB)
[master]$ wwsh -y node set c1 -D ib0 --ipaddr=<c1_ipoib> --netmask=<ipoib_netmask>
[master]$ wwsh -y node set c2 -D ib0 --ipaddr=<c2_ipoib> --netmask=<ipoib_netmask>
[master]$ wwsh -y node set c3 -D ib0 --ipaddr=<c3_ipoib> --netmask=<ipoib_netmask>
[master]$ wwsh -y node set c4 -D ib0 --ipaddr=<c4_ipoib> --netmask=<ipoib_netmask>
[master]$ wwsh -y provision set c[1-4] --fileadd=ifcfg-ib0.ww

# Restart dhcp / update PXE
[master]$ service dhcpd restart
[master]$ wwsh pxe update
\end{lstlisting}
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - optionally add 4 more hosts
% [master]$ if [[ $USE_DYNAMIC_HOSTNUM -eq 1 && $NUM_COMPUTES -ge 8 ]];then
% [master]$    wwsh -y node new c5 --ipaddr=<c5_ip> --hwaddr=<c5_mac> 
% [master]$    wwsh -y node new c6 --ipaddr=<c6_ip> --hwaddr=<c6_mac> 
% [master]$    wwsh -y node new c7 --ipaddr=<c7_ip> --hwaddr=<c7_mac> 
% [master]$    wwsh -y node new c8 --ipaddr=<c8_ip> --hwaddr=<c8_mac> 
% [master]$    wwsh -y node set c5 -D ib0 --ipaddr=<c5_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c6 -D ib0 --ipaddr=<c6_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c7 -D ib0 --ipaddr=<c7_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c8 -D ib0 --ipaddr=<c8_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y provision set c[5-8] --vnfs=sles12 --bootstrap=`uname -r` --files=dynamic_hosts,passwd,group,shadow,munge.key 
% [master]$    wwsh -y provision set c[5-8] --fileadd=ifcfg-ib0.ww     
% [master]$    service dhcpd restart
% [master]$    wwsh pxe update
% [master]$ fi
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - optionally add 8 more hosts
% [master]$ if [[ $USE_DYNAMIC_HOSTNUM -eq 1 && $NUM_COMPUTES -ge 16 ]];then
% [master]$    wwsh -y node new c9  --ipaddr=<c9_ip>  --hwaddr=<c9_mac> 
% [master]$    wwsh -y node new c10 --ipaddr=<c10_ip> --hwaddr=<c10_mac> 
% [master]$    wwsh -y node new c11 --ipaddr=<c11_ip> --hwaddr=<c11_mac> 
% [master]$    wwsh -y node new c12 --ipaddr=<c12_ip> --hwaddr=<c12_mac> 

% [master]$    wwsh -y node new c13 --ipaddr=<c13_ip> --hwaddr=<c13_mac> 
% [master]$    wwsh -y node new c14 --ipaddr=<c14_ip> --hwaddr=<c14_mac> 
% [master]$    wwsh -y node new c15 --ipaddr=<c15_ip> --hwaddr=<c15_mac> 
% [master]$    wwsh -y node new c16 --ipaddr=<c16_ip> --hwaddr=<c16_mac> 

% [master]$    wwsh -y node set c9  -D ib0 --ipaddr=<c9_ipoib>  --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c10 -D ib0 --ipaddr=<c10_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c11 -D ib0 --ipaddr=<c11_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c12 -D ib0 --ipaddr=<c12_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c13 -D ib0 --ipaddr=<c13_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c14 -D ib0 --ipaddr=<c14_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c15 -D ib0 --ipaddr=<c15_ipoib> --netmask=<ipoib_netmask>
% [master]$    wwsh -y node set c16 -D ib0 --ipaddr=<c16_ipoib> --netmask=<ipoib_netmask>

% [master]$    wwsh -y provision set c* --vnfs=sles12 --bootstrap=`uname -r` --files=dynamic_hosts,passwd,group,shadow,munge.key 
% [master]$    wwsh -y provision set c* --fileadd=ifcfg-ib0.ww     
% [master]$    service dhcpd restart
% [master]$    wwsh pxe update
% [master]$ fi
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - time update
% [master]$ TIME_T2=$(date +%s) ; TIME_DELTA=$(($TIME_T2-$TIME_T1)) ; TIME_T1=$TIME_T2
% [master]$ echo " "; echo "END TIMING = $TIME_DELTA (secs)"
% end_fsp_run

\subsection{Boot compute nodes}

\input{../../common/reset_computes} 
The following commands use the \texttt{ipmitool} utility to initiate power
resets on each of the four compute hosts. Note that the utility requires that
the \texttt{IPMI\_PASSWORD} environment variable be set with the local BMC password in
order to work interactively.

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ipmitool -E -I lanplus -H <c1_bmc> -U root chassis power reset   # power cycle c1
[master]$ ipmitool -E -I lanplus -H <c2_bmc> -U root chassis power reset   # power cycle c2
[master]$ ipmitool -E -I lanplus -H <c3_bmc> -U root chassis power reset   # power cycle c3
[master]$ ipmitool -E -I lanplus -H <c4_bmc> -U root chassis power reset   # power cycle c4
\end{lstlisting} 

% begin_fsp_run 
% fsp_validation_comment - optionally add 4 more hosts
% [master]$ if [[ $USE_DYNAMIC_HOSTNUM -eq 1 && $NUM_COMPUTES -ge 8 ]];then
% [master]$    ipmitool -E -I lanplus -H <c5_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c6_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c7_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c8_bmc> -U root chassis power reset
% [master]$ fi
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - optionally add 8 more hosts
% [master]$ if [[ $USE_DYNAMIC_HOSTNUM -eq 1 && $NUM_COMPUTES -ge 8 ]];then
% [master]$    ipmitool -E -I lanplus -H <c9_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c10_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c11_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c12_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c13_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c14_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c15_bmc> -U root chassis power reset
% [master]$    ipmitool -E -I lanplus -H <c16_bmc> -U root chassis power reset
% [master]$ fi
% end_fsp_run

% end_fsp_run

Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 120 seconds, and run a command on the newly imaged
compute hosts using \texttt{pdsh}, execute the following:

% begin_fsp_run
\begin{lstlisting}[language=bash]
[master]$ sleep 120
[master]$ /opt/fsp/admin/pdsh/bin/pdsh -w c[1-4] uptime
c1  05:03am  up   0:02,  0 users,  load average: 0.20, 0.13, 0.05
c2  05:03am  up   0:02,  0 users,  load average: 0.20, 0.14, 0.06
c3  05:03am  up   0:02,  0 users,  load average: 0.19, 0.15, 0.06
c4  05:03am  up   0:02,  0 users,  load average: 0.15, 0.12, 0.05
\end{lstlisting}
% end_fsp_run



\section{Install \FSP{} Development Components}

The install procedure outlined in \S\ref{sec:basic_install}
highlighted the steps necessary to install a {\em master} host,
assemble and customize a {\em compute} image, and provision several
compute hosts from bare-metal.  With these steps completed, 
%a {\em compute} Once the completion of the
%basic install procedure outlined in Section~\ref{sec:basic_install} is
%complete, 
additional \FSP{}-provided packages can now be added to support a flexible HPC
development environment including development tools, C/C++/Fortran compilers,
MPI stacks, and a variety of 3rd party libraries. The following subsections
highlight the additional software installation procedures, including the
addition of Intel licensed software (e.g. Composer compiler suite, \Intel{}
MPI). It is assumed that the end-site administrator will procure and install
the necessary licenses in order to use the Intel proprietary software.

\subsection{Development Tools}

To aid in general development efforts, \FSP{} provides recent versions of the \GNU{}
autotools collection and the {\em valgrind*} memory debugger. These can be installed as follows:

% begin_fsp_run
% fsp_validation_comment Install dev tools
\begin{lstlisting}[language=bash,keywords={},literate={-}{-}1]
[master]$ zypper -n install -t pattern fsp-autotools
[master]$ zypper -n install valgrind-fsp
\end{lstlisting}
% end_fsp_run

\subsection{Compilers}

\FSP{} presently packages two compiler families ({\GNU{}} and {\Intel{}
  Parallel Studio}) that are integrated within the underlying
modules-environment system in a hierarchical fashion. End users of a \FSP{}
system can choose to access one compiler at a time and will be presented with
additional compiler-dependent software as a function of which compiler
toolchain is currently loaded. Each compiler toolchain can be installed
separately and the following commands illustrate the installation of both along
with any necessary dependencies:

% begin_fsp_run
% fsp_validation_comment Install compilers
\begin{lstlisting}[language=bash]
[master]$ zypper -n install gnu-compilers-fsp intel-compilers-fsp
\end{lstlisting}
% end_fsp_run

\subsection{Performance Tools}

To aid in application performance analysis, \FSP{} provides a variety of
open-source and Intel licensed software. These can be installed as follows:

% begin_fsp_run
% fsp_validation_comment Install performance tools
\begin{lstlisting}[language=bash,keywords={},literate={-}{-}1]
[master]$ zypper -n install papi-fsp
[master]$ zypper -n install intel-itac-fsp
[master]$ zypper -n install intel-vtune-fsp
[master]$ zypper -n install intel-advisor-fsp
[master]$ zypper -n install intel-inspector-fsp
[master]$ zypper -n install -t pattern fsp-tau
\end{lstlisting}
% end_fsp_run

\subsection{MPI Stacks} \label{sec:mpi}

For MPI development support, \FSP{} presently provides pre-packaged builds for
three MPI families: 

\begin{itemize*}
\item \Intel{}~MPI
\item OpenMPI
\item MVAPICH2
\end{itemize*}
 For ABI consistency, each of the open-source MPI families (OpenMPI and
 MVAPICH2) is built against each of the two supported compiler families
 resulting in total of four build combinations.  The \Intel{} MPI stack is also
 configured to support both the \GNU{} and Intel compiler toolchain directly, but
 is packaged as a single RPM. Installation of all of the MPI family instances,
 can be accomplished via the following command. Note the use of wildcards
 (\texttt{*}) in this example in order to install both \GNU{} and Intel builds for
 OpenMPI and MVAPICH2.

% begin_fsp_run
% fsp_validation_comment Install MPI
\begin{lstlisting}[language=bash]
[master]$ zypper -n install  openmpi-*-fsp mvapich2-*-fsp intel-mpi-fsp
[master]$ zypper -n install -t pattern fsp-imb
\end{lstlisting}
% end_fsp_run

%\subsection{Install Licenses for Intel Software}
%For internal testing convenience, an RPM containing valid licenses for Intel
%compilers and MPI is available. To install, issue:

% begin_fsp_run
% fsp_validation_comment Install licenses
% \begin{lstlisting}[language=bash,keywords={},upquote=true]
% [master]$ zypper -n install intel_licenses
% \end{lstlisting}
% end_fsp_run

\subsection{3rd Party Libraries and Tools} \label{sec:3rdparty}

\FSP{} provides pre-packaged builds for a number of popular open-source
tools and libraries used by HPC applications and developers. For
example, \FSP{} provides builds for \FFTW{} and \hdffive{} (including serial and parallel
I/O support), and the \GNU{} Scientific Library (GSL). Again, multiple builds of
each package are available in the \FSP{} repository to support multiple compiler
and MPI family combinations where appropriate. The general naming convention
for builds provided by \FSP{} is to append the compiler and MPI family name that
the library was built against directly into the package name. For example,
libraries that do not require MPI as part of the build process adopt the
following RPM name: \\

\noindent
\texttt{package-<compiler\_family>-fsp-<package\_version>-<release>.rpm} \\

\noindent Packages that require MPI as part of the build expand upon this convention to
additionally include the MPI family name as follows: \\

\noindent
\texttt{package-<compiler\_family>-<mpi\_family>-fsp-<package\_version>-<release>.rpm} \\

To illustrate this further, the command below queries the locally configured
repositories to identify all of the available \FFTW{} packages that were built
with the \GNU{} toolchain. The resulting output that is included shows that
pre-built versions are available for each of the supported MPI families
presented in \S\ref{sec:mpi}.

\begin{lstlisting}[language=bash]
[master]$ zypper search -t package fftw-gnu-*-fsp
Loading repository data...
Reading installed packages...

S | Name                  | Summary                          | Type   
--+-----------------------+----------------------------------+--------
  | fftw-gnu-impi-fsp     | A Fast Fourier Transform library | package
  | fftw-gnu-mvapich2-fsp | A Fast Fourier Transform library | package
  | fftw-gnu-openmpi-fsp  | A Fast Fourier Transform library | package
\end{lstlisting}

Note that \FSP{} provided 3rd party builds are configured to be installed
into a common top-level repository so that they can be easily exported to
desired hosts within the cluster. This common top-level path
(\texttt{/opt/fsp/pub}) was previously configured to be mounted on {\em
  compute} nodes in \S\ref{sec:master_customization}, so the packages will be
immediately available for use on the cluster after installation on the {\em
  master} host.  For convenience, \FSP{} provides package aliases for these 3rd
party libraries and utilities that can be used to install all of the available
compiler/MPI family permutations. To install all of the available package
offerings within \FSP{}, issue the following:

% begin_fsp_run
% fsp_validation_comment Install 3rd party libraries
\begin{lstlisting}[language=bash,keywords={},upquote=true,keepspaces]
[master]$ zypper -n install -t pattern fsp-numpy    # adds 2 numerical python packages
[master]$ zypper -n install -t pattern fsp-scipy    # adds 6 scientific python packages
[master]$ zypper -n install -t pattern fsp-hdf5     # adds 2 HDF5 packages
[master]$ zypper -n install -t pattern fsp-netcdf   # adds 6 NetCDF packages
[master]$ zypper -n install -t pattern fsp-gsl      # adds 2 GSL packages
[master]$ zypper -n install -t pattern fsp-metis    # adds 2 METIS packages
[master]$ zypper -n install -t pattern fsp-fftw     # adds 6 FFTW packages
[master]$ zypper -n install -t pattern fsp-phdf5    # adds 6 (parallel) HDF5 packages
[master]$ zypper -n install -t pattern fsp-petsc    # adds 6 PETSC packages
[master]$ zypper -n install -t pattern fsp-boost    # adds 6 Boost packages
\end{lstlisting}
% end_fsp_run

% begin_fsp_run 
% fsp_validation_comment - time update
% [master]$ TIME_T2=$(date +%s) ; TIME_DELTA=$(($TIME_T2-$TIME_T1)) ; TIME_T1=$TIME_T2
% [master]$ echo " "; echo "END TIMING = $TIME_DELTA (secs)"
% end_fsp_run

\section{Resource Manager Startup}

In section \S\ref{sec:basic_install}, the \SLURM{} resource manager was installed
and configured for use on both the {\em master} host and {\em compute} node
instances. With the cluster nodes up and functional, we can now startup the
resource manager services in preparation for running user jobs. Generally, this
is a two-step process that requires starting up the controller daemons on the {\em
  master} host and the client daemons on each of the {\em compute} hosts.  
%Since the {\em compute} hosts were booted into an image that included the SLURM client
%components, the daemons should already be running on the {\em compute}
%hosts. 
Note that \SLURM{} leverages the use of the {\em munge} library to provide
authentication services and this daemon also needs to be running on all hosts
requiring within the resource management pool. 
%The munge daemons should already
%be running on the {\em compute} subsystem at this point, 
The following commands can be used to startup the necessary services to support
resource management under \SLURM{}.
%,  onso the steps required
%to startup the required services on the {\em master} host are as follows: 

% begin_fsp_run
% fsp_validation_comment Startup RM
\begin{lstlisting}[language=bash]
# start munge and slurm controller on master host
[master]$ service munge start
[master]$ service slurmctld start

#  start munge and slurm clients on compute hosts
[master]$ /opt/fsp/admin/pdsh/bin/pdsh -w c[1-4] service munge start
[master]$ /opt/fsp/admin/pdsh/bin/pdsh -w c[1-4] service slurmd start
\end{lstlisting}
% end_fsp_run

In the default configuration, the {\em compute} hosts will be initialized in an
{\em unknown} state. To place the hosts into production such that they are
eligible to schedule user jobs, issue the following:

% begin_fsp_run
% fsp_validation_comment - Sleep for 5 seconds (CI only command)
% [master]$ sleep 5
% end_fsp_run

% begin_fsp_run
\begin{lstlisting}[language=bash]
[master]$ scontrol update nodename=c[1-4] state=idle
\end{lstlisting}
% end_fsp_run


\section{Run a Test Job}

With the resource manager enabled for production usage, users should now be
able to run jobs.  Recall that we added a ``test'' user on the {\em master}
host in \S\ref{sec:add_rm} that can now be used to run an example test job.
\FSP{} includes a simple ``hello-world'' MPI application in the
\texttt{/opt/fsp/pub/examples} directory that can be used for this quick
compilation and execution.  To use the test account to compile and execute the
application interactively through the resource manager, execute the following:

\begin{lstlisting}[language=bash,keywords={}]
# switch to "test" user
[master]$ su - test

# Compile and execute with resource manager
[test@master ~]$ mpicc -o hello -O3 /opt/fsp/pub/examples/mpi/hello.c

[test@master ~]$ srun -n 8 -N 2 ./hello

Hello, world (8 procs total)
    --> Process #   4 of   8 is alive. -> c2
    --> Process #   0 of   8 is alive. -> c1
    --> Process #   5 of   8 is alive. -> c2
    --> Process #   1 of   8 is alive. -> c1
    --> Process #   6 of   8 is alive. -> c2
    --> Process #   2 of   8 is alive. -> c1
    --> Process #   7 of   8 is alive. -> c2
    --> Process #   3 of   8 is alive. -> c1
\end{lstlisting}

\input{manifest}

% begin_fsp_run 
% fsp_validation_comment - time update
% [master]$ TIME_T2=$(date +%s) ; TIME_DELTA=$(($TIME_T2-$TIME_T1)) ; TIME_T1=$TIME_T2
% [master]$ echo " "; echo "END TIMING = $TIME_DELTA (secs)"
% end_fsp_run

\end{document}

