\documentclass[letterpaper]{article}
\usepackage{../../common/ohpc-doc}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

% Include git variables
\input{vc.tex}

% Title


% Define Base OS and other local macros
\newcommand{\baseOS}{SLES12}
\newcommand{\OSRepo}{SLES12}
\newcommand{\IMAGE}{sles12}
\newcommand{\install}{zypper -n install}
\newcommand{\chrootinstall}{zypper -n --root \$CHROOT install}
\newcommand{\groupinstall}{zypper -n install -t pattern}
\newcommand{\groupchrootinstall}{zypper -n --root \$CHROOT install -t pattern}

\begin{document}
\graphicspath{{../../common/figures/}}
\thispagestyle{empty}

% Title Page

\lhead{ \small {\color{logodarkgrey}\fontfamily{phv}\selectfont { Install Guide} - {\baseOS{} Version} (v\OHPCVersion{}) } \vspace*{0pt} }

{\hspace*{4in} \includegraphics[width=1.7in]{ohpc_logo_blue.pdf}}

\vspace*{2cm}
\noindent {\LARGE \color{logodarkgrey} \fontfamily{phv}\selectfont OpenHPC (v\OHPCVersion{})} \vspace*{0.1cm} \\
\noindent {\LARGE \color{logodarkgrey} \fontfamily{phv}\selectfont Cluster Building Recipes} \\ 

{\color{logoblue}\noindent\rule{6.15in}{1.2pt}} \\ \vspace{0.2cm}

\noindent {\Large \color{logodarkgrey} \fontfamily{phv}\selectfont \baseOS{} Base OS} \vspace{0.2cm}

\noindent {\large \color{logodarkgrey} \fontfamily{phv}\selectfont {\em Base Linux* Edition }}

\vspace*{4in}


%\noindent{\normalsize \color{black} Intel Cluster Makers} \vspace*{0.1cm} \\
%{\normalsize \color{logodarkgrey} Copyright~{\small\copyright}~2014-2015 Intel Corporation} \vspace*{0.1cm} \\ 
\noindent{\small \color{black} Document Last Update: \VCDateISO} \vspace*{0.1cm} \\ 
{\small \color{black} Document Revision: \VCRevision} \\ \vspace*{0.1cm}

% Disclaimer  ----------------------------------------------------
\input{../../common/legal} 

\newpage
\tableofcontents
\newpage

% Introduction  --------------------------------------------------

\section{Introduction} \label{sec:introduction}
\input{../../common/install_header}
\input{../../common/intro} \\

\input{../../common/base_edition/edition}
\input{../../common/audience}
\input{../../common/requirements}
\input{../../common/byol}
\input{../../common/inputs}

% Base Operating System --------------------------------------------

\section{Install Base Operating System (BOS)}
\input{../../common/bos}

% ------------------------------------------------------------------

\section{Install \OHPC{} Components} \label{sec:basic_install}
\input{../../common/install_ohpc_components_intro.tex}

\subsection{Enable \OHPC{} repository for local use} \label{sec:enable_repo}
\input{../../common/enable_ohpc_repo}

%%% % begin_ohpc_run
%%% % ohpc_comment_header Enable OpenHPC repository for local use \ref{sec:enable_repo}
%%% \begin{lstlisting}[language=bash,keywords={}]
%%% [sms](*\#*) zypper addrepo ${ohpc_repo}
%%% [sms](*\#*) zypper -n --no-gpg-checks --gpg-auto-import-keys refresh
%%% \end{lstlisting}
%%% % end_ohpc_run

% Dev testing for Factory
%[sms](*\#*) zypper addrepo $OpenHPC_mirror/ForestPeak:/15.16/SLE_12_Intel/ForestPeak:15.16.repo

In addition to the \OHPC{} package repository, the {\em master} host also
requires access to the standard distro repositories in order to resolve
necessary dependencies. For \baseOS{}, the requirements are to have access to
both the base and SDK repositories:

\begin{itemize*}
\item SLES12-12-0
\item SLES12-12-sdk
\end{itemize*}

\input{../../common/automation}

\subsection{Add provisioning services on {\em master} node} \label{sec:add_provisioning}
\input{../../common/install_provisioning_intro}
\input{../../common/firewall}

% begin_ohpc_run
% ohpc_validation_comment Disable firewall 
\begin{lstlisting}[language=bash,keywords={}]
[sms](*\#*) systemctl disable SuSEfirewall2
[sms](*\#*) systemctl stop SuSEfirewall2
\end{lstlisting}
% end_ohpc_run

\input{../../common/time}

\subsection{Add resource management services on {\em master} node} \label{sec:add_rm}
\input{../../common/install_slurm}

\subsection{Add \InfiniBand{} support services on {\em master} node} \label{sec:add_ofed}

The following command adds OFED and PSM support using base distro-provided drivers
to the chosen {\em master} host.

% begin_ohpc_run
% ohpc_comment_header Add InfiniBand support services on master node \ref{sec:add_ofed}
\begin{lstlisting}[language=bash,keywords={}]
[sms](*\#*) (*\install*) libibverbs-runtime libmlx4-rdmav2 libipathverbs-rdmav2
[sms](*\#*) (*\install*) libibmad5 librdmacm1 rdma infinipath-psm dapl-devel dapl-utils 

# Provide udev rules to enable /dev/ipath access for InfiniPath devices
[sms](*\#*) cp /opt/ohpc/pub/examples/udev/60-ipath.rules /etc/udev/rules.d/

# Load IB drivers
[sms](*\#*) systemctl start rdma
\end{lstlisting}
% end_ohpc_run

With the \InfiniBand{} drivers included, you can also enable (optional) IPoIB functionality
which provides a mechanism to send IP packets over the IB network. If you plan
to mount a \Lustre{} file system over \InfiniBand{} (see \S\ref{sec:lustre_client}
for additional details), then having IPoIB enabled is a requirement for the
\Lustre{} client. \OHPC{} provides a template configuration file to aid in setting up
an {\em ib0} interface on the {\em master} host. To use, copy the template
provided and update the \texttt{\$\{sms\_ipoib\}} and
\texttt{\$\{ipoib\_netmask\}} entries to match local desired settings (alter ib0
naming as appropriate if system contains dual-ported or multiple HCAs). 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command if [ ${enable_ipoib} -eq 1 ];then
% ohpc_indent 5
% ohpc_validation_comment Enable ib0
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) cp /opt/ohpc/pub/examples/network/sles/ifcfg-ib0 /etc/sysconfig/network

# Define local IPoIB address and netmask
[sms](*\#*) perl -pi -e "s/master_ipoib/${sms_ipoib}/" /etc/sysconfig/network/ifcfg-ib0
[sms](*\#*) perl -pi -e "s/ipoib_netmask/${ipoib_netmask}/" /etc/sysconfig/network/ifcfg-ib0

# Initiate ib0
[sms](*\#*) ifup ib0
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% end_ohpc_run

\subsection{Complete basic Warewulf setup for {\em master} node} \label{sec:setup_ww}
\input{../../common/warewulf_setup}

% begin_ohpc_run
% ohpc_comment_header Complete basic Warewulf setup for master node \ref{sec:setup_ww}
%\begin{verbatim}

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]
# Configure Warewulf provisioning to use desired internal interface
[sms](*\#*) perl -pi -e "s/device = eth1/device = ${sms_eth_internal}/" /etc/warewulf/provision.conf

# Configure DHCP server to use desired internal interface
[sms](*\#*) perl -pi -e "s/^DHCPD_INTERFACE=\S+/DHCPD_INTERFACE=${sms_eth_internal}/" /etc/sysconfig/dhcpd

# Enable tftp service for compute node image distribution
[sms](*\#*) perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp

# Configure Warewulf to use the default SLES tftp location
[sms](*\#*) perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /srv/#" /etc/warewulf/provision.conf

# Update Warewulf http configuration to use the SLES version of mod_perl
[sms](*\#*) export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[sms](*\#*) perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory to support newer apache syntax
[sms](*\#*) perl -pi -e "s/cgi-bin>\$/cgi-bin>\n Require all granted/" $MODFILE
[sms](*\#*) perl -pi -e "s/Allow from all/Require all granted/" $MODFILE
[sms](*\#*) perl -ni -e "print unless /^\s+Order allow,deny/" $MODFILE

# Enable internal interface for provisioning
[sms](*\#*) ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up

# Restart/enable relevant services to support provisioning
[sms](*\#*) systemctl restart xinetd
[sms](*\#*) systemctl enable mysql.service
[sms](*\#*) systemctl restart mysql
[sms](*\#*) systemctl enable apache2.service
[sms](*\#*) systemctl restart apache2
\end{lstlisting}
%\end{verbatim}
% end_ohpc_run


\subsection{Define {\em compute} image for provisioning}

With the provisioning services enabled, the next step is to define and
customize a system image that can subsequently be used to provision one or more
{\em compute} nodes. The following subsections highlight this process.

\subsubsection{Build initial BOS image} \label{sec:assemble_bos}

The \OHPC{} build of \Warewulf{} includes specific enhancements enabling support for
\baseOS{}. The following steps illustrate the process to build a minimal, default
image for use with \Warewulf{}. We begin by creating a directory structure on the 
{\em master}host that will represent the root filesystem of the compute node. The 
default location for this example is in
\path{/opt/ohpc/admin/images/sles12}. Note that \Warewulf{} assumes access to
an external repository (mirror.centos.org) during the \texttt{wwmkchroot}
process which requires external network connectivity on your chosen {\em
 master} host. If this is not available, or if you prefer to access a locally cached
mirror, the steps below include an example to update the OS mirror used by
\texttt{wwmkchroot} to use an alternate location defined by a
\texttt{\$\{BOS\_MIRROR\}} environment variable.

% begin_ohpc_run
% ohpc_comment_header Create compute image for Warewulf \ref{sec:assemble_bos}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]
# Define chroot location
[sms](*\#*) export CHROOT=/opt/ohpc/admin/images/sles12

# Build initial chroot image
[sms](*\#*) mkdir -p -m 755 $CHROOT                        # create chroot housing dir
[sms](*\#*) mkdir -m 755 $CHROOT/dev                       # create chroot /dev dir
[sms](*\#*) mknod -m 666 $CHROOT/dev/zero c 1 5            # create /dev/zero device
[sms](*\#*) wwmkchroot sles-12 $CHROOT                     # create base image
\end{lstlisting}
% end_ohpc_run

\subsubsection{Add \OHPC{} components} \label{sec:add_components}

The \texttt{wwmkchroot} process used in the previous step is designed to
provide a minimal \baseOS{} configuration. Next, we add additional components to
include resource management client services, \InfiniBand{} drivers, and other
additional packages to support the default \OHPC{} environment. This process uses
the \texttt{chroot} command to augment the base provisioning image and will
access the BOS and \OHPC{} repositories to resolve package install requests. To
access the remote repositories by hostname (and not IP addresses), the chroot
environment needs to be updated to enable DNS resolution. Assuming that
the {\em master} host has a working DNS configuration in place, the chroot environment can
be updated with a copy of the configuration as follows:

% begin_ohpc_run
% ohpc_comment_header Add OpenHPC components to compute image \ref{sec:add_components}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
\end{lstlisting}
% end_ohpc_run

%\newpage
% begin_ohpc_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Import GPG keys for chroot repository usage
[sms](*\#*) zypper -n --root $CHROOT --no-gpg-checks --gpg-auto-import-keys refresh

# Add SLURM client support
[sms](*\#*) (*\groupchrootinstall*) ohpc-slurm-client

# Add IB support
[sms](*\#*) (*\chrootinstall*) libibverbs-runtime libmlx4-rdmav2 libipathverbs-rdmav2
[sms](*\#*) (*\chrootinstall*) libibmad5 librdmacm1 rdma infinipath-psm dapl-devel dapl-utils

# Provide udev rules to enable /dev/ipath access for InfiniPath devices
[sms](*\#*) cp /opt/ohpc/pub/examples/udev/60-ipath.rules $CHROOT/etc/udev/rules.d/

# Add Network Time Protocol (NTP) support
[sms](*\#*) (*\chrootinstall*) ntp

# Include modules user environment
[sms](*\#*) (*\chrootinstall*) lmod-ohpc

# Enable ssh access 
[sms](*\#*) chroot $CHROOT systemctl enable sshd.service

# Remove default hostname to allow WW to provision network names
[sms](*\#*) mv $CHROOT/etc/hostname $CHROOT/etc/hostname.orig
\end{lstlisting}
% end_ohpc_run

\subsubsection{Customize system configuration} \label{sec:master_customization}

Prior to assembling the image, it is advantageous to perform any additional
customizations within the chroot environment created for the desired {\em
 compute} instance. The following steps document the process to add a local
{\em ssh} key created by \Warewulf{} to support remote access, identify the
resource manager server, configure NTP for compute resources, and enable \NFS{}
mounting of a \$HOME file system and the public \OHPC{} install path
(\texttt{/opt/ohpc/pub}) that will be hosted by the {\em master} host in this
example configuration.
%The \NFS{} exporting options use an address/netmask
%combination to limit the export scope to the defined compute nodes.

% begin_ohpc_run
% ohpc_comment_header Customize system configuration \ref{sec:master_customization}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# add new cluster key to base image
[sms](*\#*) wwinit ssh_keys
[sms](*\#*) cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add NFS client mounts of /home and /opt/ohpc/pub to base image
[sms](*\#*) echo "${sms_ip}:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> $CHROOT/etc/fstab
[sms](*\#*) echo "${sms_ip}:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

# Identify resource manager hostname on {\em master} host
[sms](*\#*) perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
\end{lstlisting}
% end_ohpc_run

% begin_ohpc_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Identify resource manager hostname on master host
[sms](*\#*) perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf

# Export /home and OpenHPC public packages from master server
[sms](*\#*) echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
[sms](*\#*) echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
[sms](*\#*) exportfs -a
[sms](*\#*) systemctl restart nfsserver

# Enable NTP time service on computes and identify {\em master} host as local NTP server
[sms](*\#*) chroot $CHROOT systemctl enable ntpd
[sms](*\#*) echo "server ${sms_ip}" >> $CHROOT/etc/ntp.conf
\end{lstlisting}
% end_ohpc_run


\begin{center}
\begin{tcolorbox}[]
  \small Slurm requires enumeration of the physical hardware characteristics
  for compute nodes under its control. In particular, three configuration
  parameters combine to define consumable compute resources: {\em Sockets},
  {\em CoresPerSocket}, and {\em ThreadsPerCore}. The default configuration
  file provided via \OHPC{} assumes dual-socket, 8 cores per socket, and two
  threads per core for this 4-node example. If this does not reflect your local
  hardware, please update the configuration file at
  \path{/etc/slurm/slurm.conf} accordingly to match your particular hardware.
\end{tcolorbox}
\end{center}

% Additional commands when additional computes are requested

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment Update basic slurm configuration if additional computes defined
% ohpc_command if [ ${num_computes} -gt 4 ];then
% ohpc_command    perl -pi -e "s/^NodeName=(\S+)/NodeName=c[1-${num_computes}]/" /etc/slurm/slurm.conf
% ohpc_command    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=c[1-${num_computes}]/" /etc/slurm/slurm.conf

% ohpc_command    perl -pi -e "s/^NodeName=(\S+)/NodeName=c[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
% ohpc_command    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=c[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
% ohpc_command fi
% end_ohpc_run

\subsubsection{Additional Customizations ({\em optional})} \label{sec:addl_customizations}

This section highlights common additional customizations that
can {\em optionally} be applied to the
local cluster environment. These customizations include:

\begin{itemize*}
\item Increase memlimits to support MPI jobs over \InfiniBand{}
\item Restrict ssh access to compute resources
\item Add Cluster Checker
\item Add \Lustre{} client
\end{itemize*}

\noindent Details on the steps required for each of these customizations are
discussed further in the following sections.

\paragraph{Increase locked memory limits}

\input{../../common/memlimits} 

% begin_ohpc_run
% ohpc_comment_header Additional customizations \ref{sec:addl_customizations}
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Update memlock settings on {\em master}
[sms](*\#*) echo "* soft memlock unlimited" >> /etc/security/limits.conf
[sms](*\#*) echo "* hard memlock unlimited" >> /etc/security/limits.conf

# Update memlock settings within compute image
[sms](*\#*) echo "* soft memlock unlimited" >> $CHROOT/etc/security/limits.conf
[sms](*\#*) echo "* hard memlock unlimited" >> $CHROOT/etc/security/limits.conf
\end{lstlisting}
% end_ohpc_run

%# Enable larger locked memory limits for Slurm daemon
%[sms](*\#*) echo "ulimit -l unlimited" >> $CHROOT/etc/sysconfig/slurm

\paragraph{Enable ssh control via resource manager} 

\input{../../common/slurm_pam}

% begin_ohpc_run
% ohpc_validation_comment Enable slurm pam module
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
\end{lstlisting}
% end_ohpc_run

\paragraph{Add Cluster Checker} \label{sec:add_clck}
\input{../../common/cluster_checker}

\paragraph{Add \Lustre{} client} \label{sec:lustre_client}

\input{../../common/lustre-client}
% SLES-specific addition
Additionally, note that a default \baseOS{} environment may not allow loading of
the necessary \Lustre{} kernel modules. Consequently, the example below includes
steps which update the {\texttt /etc/modprobe.d/10-unsupported-modules.conf}
file to allow loading of the necessary modules.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command if [ ${enable_lustre_client} -eq 1 ];then
% ohpc_indent 5

% ohpc_validation_comment Install Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Add Lustre client software to {\em master} host
[sms](*\#*) (*\install*) lustre-client-ohpc lustre-client-ohpc-modules

# Update configuration to allow Lustre modules to be loaded on master host
[sms](*\#*) perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     /etc/modprobe.d/10-unsupported-modules.conf
\end{lstlisting}
% end_ohpc_run

\clearpage
% begin_ohpc_run
% ohpc_validation_comment Enable lustre in WW compute image
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Include Lustre client software in compute image
[sms](*\#*) (*\chrootinstall*) lustre-client-ohpc lustre-client-ohpc-modules

# Update configuration to allow Lustre modules to be loaded on compute hosts
[sms](*\#*) perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     $CHROOT/etc/modprobe.d/10-unsupported-modules.conf

# Include mount point and file system mount in compute image
[sms](*\#*) mkdir $CHROOT/mnt/lustre
[sms](*\#*) echo "${mgs_fs_name} /mnt/lustre lustre defaults,_netdev,localflock 0 0" >> $CHROOT/etc/fstab
\end{lstlisting}
% end_ohpc_run



The default underlying network type used by \Lustre{} is {\em tcp}. If your
external \Lustre{} file system is to be mounted using a network type other than
{\em tcp}, additional configuration files are necessary to identify the desired
network type. The example below illustrates creation of modprobe configuration files
instructing \Lustre{} to use an \InfiniBand{} network with the \textbf{o2ib} LNET driver
attached to \texttt{ib0}. Note that these modifications are made to both the
{\em master} host and {\em compute} image.

% begin_ohpc_run
% ohpc_validation_comment Enable o2ib for Lustre
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
[sms](*\#*) echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
\end{lstlisting}
% end_ohpc_run

With the \Lustre{} configuration complete, the client can be mounted on the {\em master}
host as follows:
% begin_ohpc_run
% ohpc_validation_comment mount Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) mkdir /mnt/lustre
[sms](*\#*) mount -t lustre -o localflock ${mgs_fs_name} /mnt/lustre
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% ohpc_validation_newline
% end_ohpc_run

%% [FSP-469]: disabling ORCM

%% [FSP-469] \paragraph{Enable ORCM RAS subsystem} 
%% [FSP-469] \input{../../common/orcm}
%% [FSP-469] 
%% [FSP-469] % begin_ohpc_run
%% [FSP-469] % ohpc_validation_comment Configure ORCM aggregator and DB
%% [FSP-469] \begin{lstlisting}[language=bash,keywords={},upquote=true,keepspaces]
%% [FSP-469] # Set {\em master} host as data aggregator
%% [FSP-469] [sms](*\#*) perl -pi -e "s/aggregator_hostname/localhost/" /opt/open-rcm/etc/orcm-site.xml
%% [FSP-469] [sms](*\#*) perl -pi -e "s/Servername = (<.+>)/Servername = ${sms_ip}/" /etc/orcmdb_psql.ini
%% [FSP-469] 
%% [FSP-469] # Update BMC credentials to match local values
%% [FSP-469] 
%% [FSP-469] [sms](*\#*) perl -pi -e "s/your-bmc-username/<bmc_username>/" /opt/open-rcm/etc/openmpi-mca-params.conf
%% [FSP-469] [sms](*\#*) perl -pi -e "s/your-bmc-password/<bmc_password>/" /opt/open-rcm/etc/openmpi-mca-params.conf
%% [FSP-469] 
%% [FSP-469] # Initialize postgres DB driver
%% [FSP-469] [sms](*\#*) odbcinst -i -d -f /etc/psql_odbc_driver.ini
%% [FSP-469] [sms](*\#*) odbcinst -i -s -h -f /etc/orcmdb_psql.ini 
%% [FSP-469] 
%% [FSP-469] # Initialize postgres
%% [FSP-469] [sms](*\#*) systemctl start postgresql
%% [FSP-469] 
%% [FSP-469] # Update postgres configuration files using ORCM templates
%% [FSP-469] [sms](*\#*) cp /etc/postgresql.orcm.conf /var/lib/pgsql/data/postgresql.conf
%% [FSP-469] [sms](*\#*) cp /etc/pg_hba.orcm.conf /var/lib/pgsql/data/pg_hba.conf
%% [FSP-469] 
%% [FSP-469] # Restart postgres
%% [FSP-469] [sms](*\#*) systemctl restart postgresql
%% [FSP-469] 
%% [FSP-469] # Create ORCM DB
%% [FSP-469] [sms](*\#*) sudo -u postgres psql --command "CREATE USER orcmuser WITH SUPERUSER PASSWORD 'orcmpassword';"
%% [FSP-469] [sms](*\#*) sudo -u postgres createdb -O orcmuser orcmdb
%% [FSP-469] [sms](*\#*) sudo -u postgres psql --username=orcmuser --dbname=orcmdb -f /etc/orcmdb_psql.sql
%% [FSP-469] 
%% [FSP-469] # Start ORCM on {\em master} host (requires both orcmsched and orcmd)
%% [FSP-469] [sms](*\#*) systemctl start orcmsched
%% [FSP-469] [sms](*\#*) systemctl start orcmd
%% [FSP-469] \end{lstlisting}
%% [FSP-469] % end_ohpc_run
%% [FSP-469] 
%% [FSP-469] \input{../../common/orcm_client}

\paragraph{Add stateful provisioning support}

\input{../../common/stateful}

\paragraph{Enable forwarding of system logs} \label{sec:add_syslog}

\input{../../common/syslog}


\subsubsection{Import files} \label{sec:file_import}

The \Warewulf{} system includes functionality to import arbitrary files from
the provisioning server for distribution to managed hosts. This is one way to
distribute user credentials to {\em compute} hosts. It also provides a
convenient mechanism to provide necessary global configuration files required
by services such as Slurm. To import local file-based credentials, issue the
following:

% begin_ohpc_run
% ohpc_comment_header Import files \ref{sec:file_import}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) wwsh file import /etc/passwd
[sms](*\#*) wwsh file import /etc/group
[sms](*\#*) wwsh file import /etc/shadow 
\end{lstlisting}
% \end_ohpc_run

Similarly, to import the global Slurm configuration file and the cryptographic
key that is required by the {\em munge} authentication library to be available
on every host in the resource management pool, issue the following:

% begin_ohpc_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) wwsh file import /etc/slurm/slurm.conf
[sms](*\#*) wwsh file import /etc/munge/munge.key
\end{lstlisting}
% \end_ohpc_run

Finally, to add support for controlling IPoIB interfaces, \OHPC{} includes a
template file for \Warewulf{} that can optionally be imported and used later to provision
\texttt{ib0} network settings.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command if [ ${enable_ipoib} -eq 1 ];then
% ohpc_indent 5
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) wwsh file import /opt/ohpc/pub/examples/network/sles/ifcfg-ib0.ww
[sms](*\#*) wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network/ifcfg-ib0
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% \end_ohpc_run

\input{../../common/finalize_provisioning}

% begin_ohpc_run
% ohpc_validation_comment Add hosts to cluster

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Define four compute nodes and network settings 
[sms](*\#*) wwsh -y node new ${c_name[0]} --ipaddr=${c_ip[0]} --hwaddr=${c_mac[0]} -D ${eth_provision}
[sms](*\#*) wwsh -y node new ${c_name[1]} --ipaddr=${c_ip[1]} --hwaddr=${c_mac[1]} -D ${eth_provision}
[sms](*\#*) wwsh -y node new ${c_name[2]} --ipaddr=${c_ip[2]} --hwaddr=${c_mac[2]} -D ${eth_provision}
[sms](*\#*) wwsh -y node new ${c_name[3]} --ipaddr=${c_ip[3]} --hwaddr=${c_mac[3]} -D ${eth_provision}
\end{lstlisting}
% end_ohpc_run

\begin{center}
\begin{tcolorbox}[]
\small
\Warewulf{} ships with a utility to automatically register new compute nodes.
Nodes will be added to the \Warewulf{} database in the order in which their
DHCP requests are received by the master, so care must be taken to boot nodes
in the order one wishes to see preserved in the warewulf databse. The IP
address provided will be incremented after each node is found, and the utility
will exit after all specified nodes have been found.
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily,]
[sms](*\#*) wwnodescan --netdev=${eth_provision} --ipaddr=${c_ip[0]} --netmask=${internal_netmask} \
    --vnfs=sles12 --bootstrap=`uname -r` ${c_name[0]}-${c_name[3]}
\end{lstlisting}
\end{tcolorbox}
\end{center}

% begin_ohpc_run
% ohpc_validation_comment Add hosts to cluster (Cont.)
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Optionally register additional compute nodes
[sms](*\#*) for ((i=4; i<$num_computes; i++)) ; do
              wwsh -y node new ${c_name[$i]} --ipaddr=${c_ip[$i]} --hwaddr=${c_mac[$i]} -D ${eth_provision}
           done

# Define provisioning image for hosts
[sms](*\#*) wwsh -y provision set "${compute_regex}" --vnfs=sles12 --bootstrap=`uname -r` \
     --files=dynamic_hosts,passwd,group,shadow,slurm.conf,munge.key
\end{lstlisting}

% ohpc_validation_newline
% ohpc_validation_comment Optionally, define IPoIB network settings (required if planning to mount Lustre over IB)
% ohpc_command if [ ${enable_ipoib} -eq 1 ];then
% ohpc_indent 5
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Optionally define IPoIB network settings (required if planning to mount Lustre over IB)
[sms](*\#*) for ((i=0; i<$num_computes; i++)) ; do
              wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
           done
[sms](*\#*) wwsh -y provision set "${compute_regex}" --fileadd=ifcfg-ib0.ww
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% ohpc_validation_newline

\clearpage
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Restart dhcp / update PXE
[sms](*\#*) systemctl restart dhcpd
[sms](*\#*) wwsh pxe update
\end{lstlisting}
% end_ohpc_run

% begin_ohpc_run

% ohpc_validation_comment Optionally, add arguments to bootstrap kernel
% ohpc_command if [ ${enable_kargs} ]; then
% ohpc_command    wwsh provision set "${compute_regex}" --kargs=${kargs}
% ohpc_command fi

% end_ohpc_run


\subsection{Boot compute nodes} \label{sec:boot_computes}

\input{../../common/reset_computes} 
The following commands use the \texttt{ipmitool} utility to initiate power
resets on each of the four compute hosts. Note that the utility requires that
the \texttt{IPMI\_PASSWORD} environment variable be set with the local BMC password in
order to work interactively.

% begin_ohpc_run
% ohpc_comment_header Boot compute nodes \ref{sec:boot_computes}
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) ipmitool -E -I lanplus -H ${c_bmc[0]} -U ${bmc_username} chassis power reset   # power cycle c1
[sms](*\#*) ipmitool -E -I lanplus -H ${c_bmc[1]} -U ${bmc_username} chassis power reset   # power cycle c2
[sms](*\#*) ipmitool -E -I lanplus -H ${c_bmc[2]} -U ${bmc_username} chassis power reset   # power cycle c3
[sms](*\#*) ipmitool -E -I lanplus -H ${c_bmc[3]} -U ${bmc_username} chassis power reset   # power cycle c4

# Optionally boot additional compute nodes
[sms](*\#*) for ((i=4; i<$num_computes; i++)) ; do
              ipmitool -E -I lanplus -H ${c_bmc[$i]} -U ${bmc_username} chassis power reset
           done
\end{lstlisting} 
% end_ohpc_run

Once kicked off, the boot process should take less than 5 minutes (depending on
BIOS post times) and you can verify that the compute hosts are available via
ssh, or via parallel ssh tools to multiple hosts. For example, to run a command
on the newly imaged compute hosts using \texttt{pdsh}, execute the following:

\begin{lstlisting}[language=bash]
[sms](*\#*) pdsh -w c[1-4] uptime
c1  05:03am  up   0:02,  0 users,  load average: 0.20, 0.13, 0.05
c2  05:03am  up   0:02,  0 users,  load average: 0.20, 0.14, 0.06
c3  05:03am  up   0:02,  0 users,  load average: 0.19, 0.15, 0.06
c4  05:03am  up   0:02,  0 users,  load average: 0.15, 0.12, 0.05
\end{lstlisting}

\begin{center}
\begin{tcolorbox}[]
\small
While the \texttt{[l]pxelinux.0} that ships with \texttt{warewulf-provision-ohpc}
support a wide range of hardware some hosts may boot more reliably or faster
using the BOS provided \texttt{[l]pxelinux.0} that comes from
\texttt{syslinux-tftpboot}. Replace the \texttt{[l]pxelinux.0} from 
\texttt{warewulf-provision-ohpc} version if necessary.
\end{tcolorbox}
\end{center}

\section{Install \OHPC{} Development Components}

The install procedure outlined in \S\ref{sec:basic_install}
highlighted the steps necessary to install a {\em master} host,
assemble and customize a {\em compute} image, and provision several
compute hosts from bare-metal. With these steps completed, 
%a {\em compute} Once the completion of the
%basic install procedure outlined in Section~\ref{sec:basic_install} is
%complete, 
additional \OHPC{}-provided packages can now be added to support a flexible HPC
development environment including development tools, C/C++/Fortran compilers,
MPI stacks, and a variety of 3rd party libraries. The following subsections
highlight the additional software installation procedures, including the
addition of Intel licensed software (e.g. Composer compiler suite, \Intel{}
MPI). It is assumed that the end-site administrator will procure and install
the necessary licenses in order to use the Intel proprietary software.

\subsection{Development Tools} \label{sec:install_dev_tools}
\input{../../common/dev_tools}

\subsection{Compilers} \label{sec:install_compilers}
\input{../../common/compilers}

\subsection{MPI Stacks} \label{sec:mpi}
\input{../../common/mpi}

\subsection{Performance Tools} \label{sec:install_perf_tools}
\input{../../common/perf_tools}

\subsection{Setup default development environment}
\input{../../common/default_dev}

\subsection{3rd Party Libraries and Tools} \label{sec:3rdparty}

\OHPC{} provides pre-packaged builds for a number of popular open-source
tools and libraries used by HPC applications and developers. For
example, \OHPC{} provides builds for \FFTW{} and \hdffive{} (including serial and parallel
I/O support), and the \GNU{} Scientific Library (GSL). Again, multiple builds of
each package are available in the \OHPC{} repository to support multiple compiler
and MPI family combinations where appropriate. Note, however, that not all
combinatorial permutations may be available for components where there are known
license incompatibilities. The general naming convention
for builds provided by \OHPC{} is to append the compiler and MPI family name that
the library was built against directly into the package name. For example,
libraries that do not require MPI as part of the build process adopt the
following RPM name: \\

\noindent
\texttt{package-<compiler\_family>-ohpc-<package\_version>-<release>.rpm} \\

\noindent Packages that do require MPI as part of the build expand upon this convention to
additionally include the MPI family name as follows: \\

\noindent
\texttt{package-<compiler\_family>-<mpi\_family>-ohpc-<package\_version>-<release>.rpm} \\

To illustrate this further, the command below queries the locally configured
repositories to identify all of the available \FFTW{} packages that were built
with the \GNU{} toolchain. The resulting output that is included shows that
pre-built versions are available for each of the supported MPI families
presented in \S\ref{sec:mpi}.

\begin{lstlisting}[language=bash]
[sms](*\#*) zypper search -t package fftw-gnu-*-ohpc
Loading repository data...
Reading installed packages...

S | Name                  | Summary                          | Type   
--+-----------------------+----------------------------------+--------
  | fftw-gnu-impi-ohpc     | A Fast Fourier Transform library | package
  | fftw-gnu-mvapich2-ohpc | A Fast Fourier Transform library | package
  | fftw-gnu-openmpi-ohpc  | A Fast Fourier Transform library | package
\end{lstlisting}

\begin{center}
\begin{tcolorbox}[]
\small
\OHPC{}-provided 3rd party builds are configured to be installed
into a common top-level repository so that they can be easily exported to
desired hosts within the cluster. This common top-level path
(\path{/opt/ohpc/pub}) was previously configured to be mounted on {\em
 compute} nodes in \S\ref{sec:master_customization}, so the packages will be
immediately available for use on the cluster after installation on the {\em
 master} host.
\end{tcolorbox}
\end{center}

For convenience, \OHPC{} provides package aliases for these 3rd
party libraries and utilities that can be used to install all of the available
compiler/MPI family permutations. To install all of the available package
offerings within \OHPC{}, issue the following:

\input{../../common/third_party_libs}


\vspace*{0.2cm}

\section{Resource Manager Startup} \label{sec:rms_startup}
\input{../../common/slurm_startup}

%% \begin{center}
%% \begin{tcolorbox}[]
%% \small
%% While most resource managers provide similar functionality, the syntax of
%% specific commands can vary substantially. The following table highlights some
%% common examples of those differences.
%% \vspace*{0.2cm}
%% \input{../../common/rms_equivalence_table}
%% \end{tcolorbox}
%% \end{center}


%\clearpage
\section{Run a Test Job} \label{sec:test_job}
\input{../../common/slurm_test_job}

\clearpage
\appendix
%\section*{Appendices}
{\bf \LARGE \centerline{Appendices}} \vspace*{0.2cm}

\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}

\input{../../common/automation_appendix}
\input{manifest}
\input{../../common/signature}


\end{document}

