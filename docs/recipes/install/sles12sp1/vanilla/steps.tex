\documentclass[letterpaper]{article}
\usepackage{../../common/ohpc-doc}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

% Include git variables
\input{vc.tex}

% Title


% Define Base OS and other local macros
\newcommand{\baseOS}{SLES12SP1}
\newcommand{\OSRepo}{SLE\_12\_SP1}
\newcommand{\baseos}{sles12sp1}
\newcommand{\install}{zypper -n install}
\newcommand{\chrootinstall}{zypper -n --root \$CHROOT install}
\newcommand{\groupinstall}{zypper -n install -t pattern}
\newcommand{\groupchrootinstall}{zypper -n --root \$CHROOT install -t pattern}

% boolean for os-specific formatting
\togglefalse{isCentOS}

\begin{document}
\graphicspath{{../../common/figures/}}
\thispagestyle{empty}

% Title Page

\lhead{ \small {\color{logodarkgrey}\fontfamily{phv}\selectfont { Install Guide} - {\baseOS{} Version} (v\OHPCVersion{}) } \vspace*{0pt} }

{\hspace*{4in} \includegraphics[width=1.7in]{ohpc_logo_blue.pdf}}

\vspace*{2cm}
\noindent {\LARGE \color{logodarkgrey} \fontfamily{phv}\selectfont OpenHPC (v\OHPCVersion{})} \vspace*{0.1cm} \\
\noindent {\LARGE \color{logodarkgrey} \fontfamily{phv}\selectfont Cluster Building Recipes} \\ 

{\color{logoblue}\noindent\rule{6.15in}{1.2pt}} \\ \vspace{0.2cm}

\noindent {\Large \color{logodarkgrey} \fontfamily{phv}\selectfont \baseOS{} Base OS} \vspace{0.2cm}

\noindent {\large \color{logodarkgrey} \fontfamily{phv}\selectfont {\em Base Linux* Edition }}

\vspace*{4in}

\noindent{\small \color{black} Document Last Update: \VCDateISO} \vspace*{0.1cm} \\ 
{\small \color{black} Document Revision: \VCRevision} \\ \vspace*{0.1cm}

% Disclaimer  ----------------------------------------------------
\input{../../common/legal} 

\newpage
\tableofcontents
\newpage

% Introduction  --------------------------------------------------

\section{Introduction} \label{sec:introduction}
\input{../../common/install_header}
\input{../../common/intro} \\

\input{../../common/base_edition/edition}
\input{../../common/audience}
\input{../../common/requirements}
\input{../../common/inputs}

% Base Operating System --------------------------------------------

\section{Install Base Operating System (BOS)}
\input{../../common/bos}

% ------------------------------------------------------------------

\section{Install \OHPC{} Components} \label{sec:basic_install}
\input{../../common/install_ohpc_components_intro.tex}

\subsection{Enable \OHPC{} repository for local use} \label{sec:enable_repo}
\input{../../common/enable_ohpc_repo}

In addition to the \OHPC{} package repository, the {\em master} host also
requires access to the standard distro repositories in order to resolve
necessary dependencies. For \baseOS{}, the requirements are to have access to
both the base and SDK repositories:

\begin{itemize*}
\item SLES12-SP1-12.1-0
\item SLES12-SP1-SDK-12.1-0
\end{itemize*}

\input{../../common/automation}

\subsection{Add provisioning services on {\em master} node} \label{sec:add_provisioning}
\input{../../common/install_provisioning_intro}
\input{../../common/firewall}

% begin_ohpc_run
% ohpc_validation_comment Disable firewall 
\begin{lstlisting}[language=bash,keywords={}]
[sms](*\#*) systemctl disable SuSEfirewall2
[sms](*\#*) systemctl stop SuSEfirewall2
\end{lstlisting}
% end_ohpc_run

\input{../../common/enable_pxe}

\input{../../common/time}

\subsection{Add resource management services on {\em master} node} \label{sec:add_rm}
\input{../../common/install_slurm}

\subsection{Add \InfiniBand{} support services on {\em master} node} \label{sec:add_ofed}

The following command adds OFED and PSM support using base distro-provided drivers
to the chosen {\em master} host.

% begin_ohpc_run
% ohpc_comment_header Add InfiniBand support services on master node \ref{sec:add_ofed}
\begin{lstlisting}[language=bash,keywords={}]
[sms](*\#*) (*\install*) libibverbs-runtime libmlx4-rdmav2 libipathverbs-rdmav2
[sms](*\#*) (*\install*) libibmad5 librdmacm1 rdma infinipath-psm dapl-devel dapl-utils 

# Provide udev rules to enable /dev/ipath access for InfiniPath devices
[sms](*\#*) cp /opt/ohpc/pub/examples/udev/60-ipath.rules /etc/udev/rules.d/

# Load IB drivers
[sms](*\#*) systemctl start rdma
\end{lstlisting}
% end_ohpc_run

With the \InfiniBand{} drivers included, you can also enable (optional) IPoIB functionality
which provides a mechanism to send IP packets over the IB network. If you plan
to mount a \Lustre{} file system over \InfiniBand{} (see \S\ref{sec:lustre_client}
for additional details), then having IPoIB enabled is a requirement for the
\Lustre{} client. \OHPC{} provides a template configuration file to aid in setting up
an {\em ib0} interface on the {\em master} host. To use, copy the template
provided and update the \texttt{\$\{sms\_ipoib\}} and
\texttt{\$\{ipoib\_netmask\}} entries to match local desired settings (alter ib0
naming as appropriate if system contains dual-ported or multiple HCAs). 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command if [[ ${enable_ipoib} -eq 1 ]];then
% ohpc_indent 5
% ohpc_validation_comment Enable ib0
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) cp /opt/ohpc/pub/examples/network/sles/ifcfg-ib0 /etc/sysconfig/network

# Define local IPoIB address and netmask
[sms](*\#*) perl -pi -e "s/master_ipoib/${sms_ipoib}/" /etc/sysconfig/network/ifcfg-ib0
[sms](*\#*) perl -pi -e "s/ipoib_netmask/${ipoib_netmask}/" /etc/sysconfig/network/ifcfg-ib0

# Initiate ib0
[sms](*\#*) ifup ib0
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% end_ohpc_run

\subsection{Complete basic Warewulf setup for {\em master} node} \label{sec:setup_ww}
\input{../../common/warewulf_setup}

% begin_ohpc_run
% ohpc_comment_header Complete basic Warewulf setup for master node \ref{sec:setup_ww}
%\begin{verbatim}

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]
# Configure Warewulf provisioning to use desired internal interface
[sms](*\#*) perl -pi -e "s/device = eth1/device = ${sms_eth_internal}/" /etc/warewulf/provision.conf

# Configure DHCP server to use desired internal interface
[sms](*\#*) perl -pi -e "s/^DHCPD_INTERFACE=\S+/DHCPD_INTERFACE=${sms_eth_internal}/" /etc/sysconfig/dhcpd

# Enable tftp service for compute node image distribution
[sms](*\#*) perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp

# Configure Warewulf to use the default SLES tftp location
[sms](*\#*) perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /srv/#" /etc/warewulf/provision.conf

# Update Warewulf http configuration to use the SLES version of mod_perl
[sms](*\#*) export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[sms](*\#*) perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory to support newer apache syntax
[sms](*\#*) perl -pi -e "s/cgi-bin>\$/cgi-bin>\n Require all granted/" $MODFILE
[sms](*\#*) perl -pi -e "s/Allow from all/Require all granted/" $MODFILE
[sms](*\#*) perl -ni -e "print unless /^\s+Order allow,deny/" $MODFILE

# Enable internal interface for provisioning
[sms](*\#*) ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up

# Restart/enable relevant services to support provisioning
[sms](*\#*) systemctl restart xinetd
[sms](*\#*) systemctl enable mysql.service
[sms](*\#*) systemctl restart mysql
[sms](*\#*) systemctl enable apache2.service
[sms](*\#*) systemctl restart apache2
\end{lstlisting}
%\end{verbatim}
% end_ohpc_run


\subsection{Define {\em compute} image for provisioning}

With the provisioning services enabled, the next step is to define and
customize a system image that can subsequently be used to provision one or more
{\em compute} nodes. The following subsections highlight this process.

\subsubsection{Build initial BOS image} \label{sec:assemble_bos}

The \OHPC{} build of \Warewulf{} includes specific enhancements enabling support for
\baseOS{}. The following steps illustrate the process to build a minimal, default
image for use with \Warewulf{}. We begin by creating a directory structure on the 
{\em master} host that will represent the root filesystem of the compute node. The 
default location for this example is in
\texttt{/opt/ohpc/admin/images/\baseos{}}.

\begin{center}
  \begin{tcolorbox}[]
    \small Note that \Warewulf{} requires access to a zypper repository during the 
    \texttt{wwmkchroot} process. The easiest way to provide this is to mount 
    the SUSE-SLE-12-SP1-Server ISO image on a web server that can communicate 
    with the {\em master} host, set the \texttt{\$\{BOS\_MIRROR\}} environment 
    variable to that URL, and update the template file {\em prior} to running 
    the \texttt{wwmkchroot} command below. For example:

% begin_ohpc_run
% ohpc_command if [ ! -z ${BOS_MIRROR+x} ]; then
% ohpc_indent 5
\begin{lstlisting}[language=bash,keywords={}]
# Override default OS repository - set BOS_MIRROR variable to desired repo location
[sms](*\#*) perl -pi -e "s#^ZYP_MIRROR=(\S+)#ZYP_MIRROR=${BOS_MIRROR}#" \
   /usr/libexec/warewulf/wwmkchroot/sles-12.tmpl
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% end_ohpc_run

\end{tcolorbox}
\end{center}

% begin_ohpc_run
% ohpc_comment_header Create compute image for Warewulf \ref{sec:assemble_bos}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true,keepspaces]
# Define chroot location
[sms](*\#*) export CHROOT=/opt/ohpc/admin/images/sles12sp1

# Build initial chroot image
[sms](*\#*) mkdir -p -m 755 $CHROOT                        # create chroot housing dir
[sms](*\#*) mkdir -m 755 $CHROOT/dev                       # create chroot /dev dir
[sms](*\#*) mknod -m 666 $CHROOT/dev/zero c 1 5            # create /dev/zero device
[sms](*\#*) wwmkchroot sles-12 $CHROOT                     # create base image
\end{lstlisting}
% end_ohpc_run

\subsubsection{Add \OHPC{} components} \label{sec:add_components}

\input{../../common/add_to_compute_chroot_intro}

\noindent Now, we can include additional components to the compute instance using
\texttt{zypper} to install into the chroot location defined previously:

\clearpage
% begin_ohpc_run
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Import GPG keys for chroot repository usage
[sms](*\#*) zypper -n --root $CHROOT --no-gpg-checks --gpg-auto-import-keys refresh

# Add SLURM client support
[sms](*\#*) (*\groupchrootinstall*) ohpc-slurm-client

# Add IB support
[sms](*\#*) (*\chrootinstall*) libibverbs-runtime libmlx4-rdmav2 libipathverbs-rdmav2
[sms](*\#*) (*\chrootinstall*) libibmad5 librdmacm1 rdma infinipath-psm dapl-devel dapl-utils

# Provide udev rules to enable /dev/ipath access for InfiniPath devices
[sms](*\#*) cp /opt/ohpc/pub/examples/udev/60-ipath.rules $CHROOT/etc/udev/rules.d/

# Add Network Time Protocol (NTP) support
[sms](*\#*) (*\chrootinstall*) ntp

# Add kernel drivers
[sms](*\#*) (*\chrootinstall*) kernel-default

# Include modules user environment
[sms](*\#*) (*\chrootinstall*) lmod-ohpc

# Enable ssh access 
[sms](*\#*) chroot $CHROOT systemctl enable sshd.service

# Remove default hostname to allow WW to provision network names
[sms](*\#*) mv $CHROOT/etc/hostname $CHROOT/etc/hostname.orig
\end{lstlisting}
% end_ohpc_run

\subsubsection{Customize system configuration} \label{sec:master_customization}

Prior to assembling the image, it is advantageous to perform any additional
customization within the chroot environment created for the desired {\em
 compute} instance. The following steps document the process to add a local
{\em ssh} key created by \Warewulf{} to support remote access, identify the
resource manager server, configure NTP for compute resources, and enable \NFS{}
mounting of a \$HOME file system and the public \OHPC{} install path
(\texttt{/opt/ohpc/pub}) that will be hosted by the {\em master} host in this
example configuration.
%The \NFS{} exporting options use an address/netmask
%combination to limit the export scope to the defined compute nodes.

% begin_ohpc_run
% ohpc_comment_header Customize system configuration \ref{sec:master_customization}
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# add new cluster key to base image
[sms](*\#*) wwinit ssh_keys
[sms](*\#*) cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add NFS client mounts of /home and /opt/ohpc/pub to base image
[sms](*\#*) echo "${sms_ip}:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> $CHROOT/etc/fstab
[sms](*\#*) echo "${sms_ip}:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

# Identify resource manager hostname on master host
[sms](*\#*) perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf

# Export /home and OpenHPC public packages from master server
[sms](*\#*) echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
[sms](*\#*) echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
[sms](*\#*) exportfs -a
[sms](*\#*) systemctl restart nfsserver
[sms](*\#*) systemctl enable nfsserver
# Enable NTP time service on computes and identify master host as local NTP server
[sms](*\#*) chroot $CHROOT systemctl enable ntpd
[sms](*\#*) echo "server ${sms_ip}" >> $CHROOT/etc/ntp.conf
[sms](*\#*) cp /etc/ntp.keys $CHROOT/etc/ntp.keys

\end{lstlisting}
% end_ohpc_run


\begin{center}
\begin{tcolorbox}[]
  \small Slurm requires enumeration of the physical hardware characteristics
  for compute nodes under its control. In particular, three configuration
  parameters combine to define consumable compute resources: {\em Sockets},
  {\em CoresPerSocket}, and {\em ThreadsPerCore}. The default configuration
  file provided via \OHPC{} assumes dual-socket, 8 cores per socket, and two
  threads per core for this 4-node example. If this does not reflect your local
  hardware, please update the configuration file at
  \path{/etc/slurm/slurm.conf} accordingly to match your particular hardware.
\end{tcolorbox}
\end{center}

% Additional commands when additional computes are requested

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment Update basic slurm configuration if additional computes defined
% ohpc_command if [ ${num_computes} -gt 4 ];then
% ohpc_command    perl -pi -e "s/^NodeName=(\S+)/NodeName=${compute_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf
% ohpc_command    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${compute_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf

% ohpc_command    perl -pi -e "s/^NodeName=(\S+)/NodeName=${compute_prefix}[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
% ohpc_command    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${compute_prefix}[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
% ohpc_command fi
% end_ohpc_run

\subsubsection{Additional Customization ({\em optional})} \label{sec:addl_customizations}
\input{../../common/compute_customizations_intro}

\paragraph{Increase locked memory limits}
\input{../../common/memlimits} 

\paragraph{Enable ssh control via resource manager} 
\input{../../common/slurm_pam}

\paragraph{Add \Lustre{} client} \label{sec:lustre_client}
\input{../../common/lustre-client}
% SLES-specific addition
Additionally, note that a default \baseOS{} environment may not allow loading of
the necessary \Lustre{} kernel modules. Consequently, the example below includes
steps which update the {\texttt /etc/modprobe.d/10-unsupported-modules.conf}
file to allow loading of the necessary modules.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment Enable Optional packages
% ohpc_validation_newline
% ohpc_command if [[ ${enable_lustre_client} -eq 1 ]];then
% ohpc_indent 5

% ohpc_validation_comment Install Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Add Lustre client software to master host
[sms](*\#*) (*\install*) lustre-client-ohpc lustre-client-ohpc-modules

# Update configuration to allow Lustre modules to be loaded on master host
[sms](*\#*) perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     /etc/modprobe.d/10-unsupported-modules.conf
\end{lstlisting}
% end_ohpc_run


% begin_ohpc_run
% ohpc_validation_comment Enable lustre in WW compute image
\begin{lstlisting}[language=bash,keywords={},upquote=true]
# Include Lustre client software in compute image
[sms](*\#*) (*\chrootinstall*) lustre-client-ohpc lustre-client-ohpc-modules

# Update configuration to allow Lustre modules to be loaded on compute hosts
[sms](*\#*) perl -pi -e "s/^allow_unsupported_modules 0/allow_unsupported_modules 1/" \
     $CHROOT/etc/modprobe.d/10-unsupported-modules.conf

# Include mount point and file system mount in compute image
[sms](*\#*) mkdir $CHROOT/mnt/lustre
[sms](*\#*) echo "${mgs_fs_name} /mnt/lustre lustre defaults,_netdev,localflock 0 0" >> $CHROOT/etc/fstab
\end{lstlisting}
% end_ohpc_run

The default underlying network type used by \Lustre{} is {\em tcp}. If your
external \Lustre{} file system is to be mounted using a network type other than
{\em tcp}, additional configuration files are necessary to identify the desired
network type. The example below illustrates creation of modprobe configuration files
instructing \Lustre{} to use an \InfiniBand{} network with the \textbf{o2ib} LNET driver
attached to \texttt{ib0}. Note that these modifications are made to both the
{\em master} host and {\em compute} image.

% begin_ohpc_run
% ohpc_validation_comment Enable o2ib for Lustre
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
[sms](*\#*) echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
\end{lstlisting}
% end_ohpc_run

With the \Lustre{} configuration complete, the client can be mounted on the {\em master}
host as follows:
% begin_ohpc_run
% ohpc_validation_comment mount Lustre client on master
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[sms](*\#*) mkdir /mnt/lustre
[sms](*\#*) mount -t lustre -o localflock ${mgs_fs_name} /mnt/lustre
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% ohpc_validation_newline
% end_ohpc_run

\clearpage
\paragraph{Add \Nagios{} monitoring}
\input{../../common/nagios}

\clearpage
\paragraph{Add \Ganglia{} monitoring}
\input{../../common/ganglia}

\paragraph{Add \clustershell{}}
\input{../../common/clustershell}

\paragraph{Add \mrsh{}}
\input{../../common/mrsh}

\paragraph{Add \genders{}}
\input{../../common/genders}

%% \paragraph{Add \powerman{}}
%% \input{../../common/powerman}

\paragraph{Add \conman{}} \label{sec:add_conman}
\input{../../common/conman}

\clearpage
\paragraph{Enable forwarding of system logs} \label{sec:add_syslog}
\input{../../common/syslog}

\subsubsection{Import files} \label{sec:file_import}
\input{../../common/import_ww_files}

%----------------
% SLES specific
%----------------

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command if [[ ${enable_ipoib} -eq 1 ]];then
% ohpc_indent 5
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[sms](*\#*) wwsh file import /opt/ohpc/pub/examples/network/sles/ifcfg-ib0.ww
[sms](*\#*) wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network/ifcfg-ib0
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% \end_ohpc_run

\input{../../common/finalize_provisioning}

%\clearpage
% begin_ohpc_run
% ohpc_validation_comment Add hosts to cluster
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Add nodes to Warewulf data store
[sms](*\#*) for ((i=0; i<$num_computes; i++)) ; do
                wwsh -y node new ${c_name[i]} --ipaddr=${c_ip[i]} --hwaddr=${c_mac[i]} -D ${eth_provision}
        done
\end{lstlisting}
% end_ohpc_run

% begin_ohpc_run
% ohpc_validation_comment Add hosts to cluster (Cont.)
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Define provisioning image for hosts
[sms](*\#*) wwsh -y provision set "${compute_regex}" --vnfs=sles12sp1 --bootstrap=`uname -r` \
     --files=dynamic_hosts,passwd,group,shadow,slurm.conf,munge.key
\end{lstlisting}

% ohpc_validation_newline
% ohpc_validation_comment Optionally, define IPoIB network settings (required if planning to mount Lustre over IB)
% ohpc_command if [[ ${enable_ipoib} -eq 1 ]];then
% ohpc_indent 5
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Optionally define IPoIB network settings (required if planning to mount Lustre over IB)
[sms](*\#*) for ((i=0; i<$num_computes; i++)) ; do
              wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
        done
[sms](*\#*) wwsh -y provision set "${compute_regex}" --fileadd=ifcfg-ib0.ww
\end{lstlisting}
% ohpc_indent 0
% ohpc_command fi
% ohpc_validation_newline
% end_ohpc_run

\input{../../common/wwnodescan}

% begin_ohpc_run
\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Restart dhcp / update PXE
[sms](*\#*) systemctl restart dhcpd
[sms](*\#*) wwsh pxe update
\end{lstlisting}
% end_ohpc_run

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment Optionally, add arguments to bootstrap kernel
% ohpc_command if [[ ${enable_kargs} ]]; then
% ohpc_command    wwsh provision set "${compute_regex}" --kargs=${kargs}
% ohpc_command fi

% end_ohpc_run

\subsubsection{Optional kernel arguments} \label{sec:optional_kargs}
\input{../../common/conman_post}

\subsubsection{Optionally configure stateful provisioning}
\input{../../common/stateful}

%\clearpage
\subsection{Boot compute nodes} \label{sec:boot_computes}
\input{../../common/reset_computes}

%----------------
% SLES specific
%----------------

\begin{center}
\begin{tcolorbox}[]
\small While the \texttt{pxelinux.0} and \texttt{lpxelinux.0} files that ship
with Warewulf to enable network boot support a wide range of hardware, some
hosts may boot more reliably or faster using the BOS versions provided via the
\texttt{syslinux} package. If you encounter PXE issues, consider
replacing the \texttt{pxelinux.0} and \texttt{lpxelinux.0} files supplied with
\texttt{warewulf-provision-ohpc} with versions from \texttt{syslinux}.
\end{tcolorbox}
\end{center}

\section{Install \OHPC{} Development Components}
\input{../../common/dev_intro.tex}

\subsection{Development Tools} \label{sec:install_dev_tools}
\input{../../common/dev_tools}

\subsection{Compilers} \label{sec:install_compilers}
\input{../../common/compilers}

\subsection{MPI Stacks} \label{sec:mpi}
\input{../../common/mpi}

\subsection{Performance Tools} \label{sec:install_perf_tools}
\input{../../common/perf_tools}

\subsection{Setup default development environment}
\input{../../common/default_dev}

\subsection{3rd Party Libraries and Tools} \label{sec:3rdparty}
\input{../../common/third_party_libs_intro}

%----------------
% SLES specific
%----------------

\begin{lstlisting}[language=bash,keepspaces=true,keywords={}]
[sms](*\#*) zypper search -t package petsc-gnu-*-ohpc
Loading repository data...
Reading installed packages...

S | Name                    | Summary
--+-------------------------+--------------------------------------------------------+--------
i | petsc-gnu-mvapich2-ohpc | Portable Extensible Toolkit for Scientific Computation | package
i | petsc-gnu-openmpi-ohpc  | Portable Extensible Toolkit for Scientific Computation | package
\end{lstlisting}

\input{../../common/third_party_libs}

\subsection{Optional Development Tool Builds} \label{sec:3rdparty_intel}
\input{../../common/pxse_enabled_builds}

\section{Resource Manager Startup} \label{sec:rms_startup}
\input{../../common/slurm_startup}

%\clearpage
\section{Run a Test Job} \label{sec:test_job}
\input{../../common/slurm_test_job}

\clearpage
\appendix
{\bf \LARGE \centerline{Appendices}} \vspace*{0.2cm}

\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}

\input{../../common/automation_appendix}
\input{../../common/rpmbuild_appendix}
\input{manifest}
\input{../../common/signature}


\end{document}

